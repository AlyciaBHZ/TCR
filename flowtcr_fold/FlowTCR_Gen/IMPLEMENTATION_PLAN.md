# Stage 2: FlowTCR-Gen Implementation Plan

> **Master Reference**: [../README.md](../README.md) (Section 4.2, Master Plan v3.1 Stage 2)
> 
> **Status**: ğŸ”„ In Progress (40%)
> 
> **Timeline**: Week 3-5 (Plan v3.1)

---

## 1. æ¨¡å—å®šä½

### 1.1 åœ¨æ•´ä½“ Pipeline ä¸­çš„è§’è‰²

```
                    Stage 1: Immuno-PLM
                              â”‚
                              â–¼
                    Top-K scaffolds + pMHC embedding
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â˜… Stage 2: FLOWTCR-GEN (You Are Here)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Topology-aware Dirichlet Flow Matching                         â”‚
â”‚  Output: CDR3Î² sequence candidates                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    Stage 3: TCRFold-Prophet
```

### 1.2 æ ¸å¿ƒç›®æ ‡

- **ç”Ÿæˆ CDR3Î²**ï¼šç»™å®š pMHC + scaffoldï¼Œç”Ÿæˆå¤šæ ·ä¸”ç‰©ç†åˆç†çš„ CDR3Î² åºåˆ—
- **æ‹“æ‰‘æ„ŸçŸ¥**ï¼šä½¿ç”¨ Collapse Token + Hierarchical Pairs ç¼–ç  TCR-pMHC æ‹“æ‰‘
- **å¯æ§ç”Ÿæˆ**ï¼šæ”¯æŒ CFG (Classifier-Free Guidance) è°ƒèŠ‚æ¡ä»¶å¼ºåº¦

### 1.3 åˆ›æ–°ç‚¹ï¼ˆè®ºæ–‡ä¸»æ‰“ï¼‰

| ç»„ä»¶ | æè¿° | åˆ›æ–°æ€§ |
|------|------|--------|
| **Collapse Token (Ïˆ)** | å¯å­¦ä¹ å…¨å±€è§‚å¯Ÿè€… | è·¨åŒºåŸŸæ³¨æ„åŠ›èšåˆ |
| **Hierarchical Pair Embeddings** | 7-level æ‹“æ‰‘ç¼–ç  | æ³¨å…¥ TCR-pMHC ç»“æ„å…ˆéªŒ |
| **Dirichlet Flow Matching** | æ°¨åŸºé…¸ simplex ä¸Šçš„è¿ç»­ç”Ÿæˆ | æ”¯æŒå¹³æ»‘æ’å€¼å’Œ CFG |

---

## 2. å½“å‰å®ç°çŠ¶æ€

### 2.1 å·²å®Œæˆ âœ…

| æ–‡ä»¶ | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|
| `flow_gen.py` | FlowMatchingModel åŸºç¡€æ¶æ„ | âœ… å¯è¿è¡Œ |
| `SinusoidalTimeEmbedding` | æ—¶é—´åµŒå…¥ | âœ… å®Œæˆ |
| `train_flow.py` | è®­ç»ƒè„šæœ¬ | âš ï¸ éœ€å‡çº§ |
| `sample.py` | ODE é‡‡æ · | âš ï¸ éœ€æ·»åŠ  CFG |
| `pipeline_impl.py` | ç«¯åˆ°ç«¯æ¨ç† | âš ï¸ éœ€æ•´åˆ Stage 1 |

### 2.2 å¾…å®ç° ğŸ”„

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | ä¾èµ– |
|------|--------|------|
| é›†æˆ `CollapseAwareEmbedding` | ğŸ”´ é«˜ | psi_model ä»£ç  |
| é›†æˆ `SequenceProfileEvoformer` | ğŸ”´ é«˜ | psi_model ä»£ç  |
| Hierarchical Pair IDs ç”Ÿæˆ | ğŸ”´ é«˜ | - |
| Dirichlet Flow (x_t æ³¨å…¥) | ğŸ”´ é«˜ | - |
| CFG å®ç° | ğŸ”´ é«˜ | - |
| Model Score Hook | ğŸŸ¡ ä¸­ | - |
| Entropy/Profile æ­£åˆ™ | ğŸŸ¡ ä¸­ | - |

### 2.3 Legacy ä»£ç ä½ç½®

```
psi_model/
â”œâ”€â”€ model.py              # â­ CollapseAwareEmbedding, SequenceProfileEvoformer
â”œâ”€â”€ model_original.py     # åŸå§‹ç‰ˆæœ¬ï¼ˆå‚è€ƒï¼‰
â””â”€â”€ train.py              # psiMonteCarloSamplerï¼ˆå‚è€ƒï¼‰
```

---

## 3. Step-by-Step Implementation Plan

### Phase 1: å¤ç”¨ psi_model ç»„ä»¶ (Day 1-3)

#### Step 1.1: ç†è§£ `CollapseAwareEmbedding`

```python
# æ¥è‡ª psi_model/model.py
class CollapseAwareEmbedding(nn.Module):
    """
    å…³é”®åŠŸèƒ½ï¼š
    1. Collapse Token (Ïˆ): å¯å­¦ä¹ çš„å…¨å±€è§‚å¯Ÿè€…
    2. Hierarchical Pair IDs: 7-level æ‹“æ‰‘å…³ç³»ç¼–ç 
    3. Region-specific weights: ä¸åŒåŒºåŸŸçš„è‡ªé€‚åº”æƒé‡
    """
    
    def create_hierarchical_pairs(self, ...):
        """
        è¿”å› pair_ids [L, L]:
        - Level 0: Ïˆ â†” Ïˆ
        - Level 1: Ïˆ â†” HD (CDR3)
        - Level 2: Ïˆ â†” æ¡ä»¶åŒºåŸŸ
        - Level 3: HD å†…éƒ¨
        - Level 4: æ¡ä»¶åŒºåŸŸå†…éƒ¨
        - Level 5: HD â†” æ¡ä»¶åŒºåŸŸ
        - Level 6: ä¸åŒæ¡ä»¶åŒºåŸŸä¹‹é—´
        """
```

#### Step 1.2: åˆ›å»º FlowTCR-Gen é€‚é…å™¨

```python
# flowtcr_fold/FlowTCR_Gen/flow_gen.py æ–°å¢

from psi_model.model import CollapseAwareEmbedding, SequenceProfileEvoformer

class FlowTCRGenEncoder(nn.Module):
    """
    å°† psi_model ç»„ä»¶é€‚é…ä¸º FlowTCR-Gen çš„æ¡ä»¶ç¼–ç å™¨
    """
    def __init__(
        self,
        s_dim: int = 256,
        z_dim: int = 64,
        n_layers: int = 6,
        vocab_size: int = 21,
        max_len: int = 512,
    ):
        super().__init__()
        
        # å¤ç”¨ psi_model çš„åµŒå…¥å±‚
        self.embedding = CollapseAwareEmbedding(
            s_in_dim=vocab_size,
            s_dim=s_dim,
            z_dim=z_dim,
            max_len=max_len,
        )
        
        # å¤ç”¨ psi_model çš„ Evoformer
        self.backbone = SequenceProfileEvoformer(
            s_dim=s_dim,
            z_dim=z_dim,
            n_layers=n_layers,
        )
    
    def forward(self, cdr3_xt, peptide, mhc, scaffold_seqs, conditioning_info):
        """
        Args:
            cdr3_xt: [B, L_cdr3, vocab] flow ä¸­é—´çŠ¶æ€
            peptide: [B, L_pep] peptide åºåˆ—
            mhc: [B, L_mhc] MHC åºåˆ—
            scaffold_seqs: Dict[str, Tensor] HV/HJ/LV/LJ åºåˆ—
            conditioning_info: List[str] ä½¿ç”¨å“ªäº›æ¡ä»¶
        
        Returns:
            s: [B, L_total, s_dim] åºåˆ—è¡¨å¾
            z: [B, L_total, L_total, z_dim] pair è¡¨å¾
        """
        # æ„å»ºè¾“å…¥å­—å…¸
        in_dict = {
            'hd': cdr3_xt,  # x_t ä½œä¸º HD åŒºåŸŸ
            'pep': peptide,
            'mhc': mhc,
            **scaffold_seqs,
        }
        
        # åµŒå…¥ + pair_ids
        s, z = self.embedding(in_dict, conditioning_info)
        
        # Evoformer å¤„ç†
        s, z = self.backbone(s, z)
        
        return s, z
```

#### Step 1.3: x_t æ³¨å…¥æ–¹å¼

```python
def inject_xt_into_embedding(self, x_t: torch.Tensor) -> torch.Tensor:
    """
    å°† flow ä¸­é—´çŠ¶æ€ x_t æ³¨å…¥åˆ°åµŒå…¥ç©ºé—´
    
    æ–¹æ³•ï¼šx_t æ˜¯ [B, L, vocab] çš„è½¯åˆ†å¸ƒ
    â†’ é€šè¿‡ embedding çŸ©é˜µçš„æœŸæœ›å¾—åˆ°è¿ç»­åµŒå…¥
    """
    # x_t: [B, L, vocab], embedding: [vocab, s_dim]
    # â†’ [B, L, s_dim]
    emb = torch.matmul(x_t, self.token_embedding.weight)
    return emb + self.position_embedding
```

---

### Phase 2: Dirichlet Flow Matching (Day 4-6)

#### Step 2.1: Flow æ’å€¼å®šä¹‰

```python
def dirichlet_interpolate(x0: torch.Tensor, x1: torch.Tensor, t: torch.Tensor):
    """
    Dirichlet Flow æ’å€¼:
    - x0: å…ˆéªŒåˆ†å¸ƒ (uniform Dirichlet æˆ–é«˜ç†µåˆ†å¸ƒ)
    - x1: ç›®æ ‡åˆ†å¸ƒ (one-hot ground truth)
    - t: æ—¶é—´ [0, 1]
    
    x_t = (1 - t) * x0 + t * x1
    """
    return (1 - t) * x0 + t * x1


def sample_x0_dirichlet(batch_size: int, seq_len: int, vocab_size: int, alpha: float = 1.0):
    """
    ä» Dirichlet(Î±, Î±, ..., Î±) é‡‡æ ·å…ˆéªŒåˆ†å¸ƒ
    Î± = 1 æ—¶ä¸ºå‡åŒ€åˆ†å¸ƒ
    """
    dist = torch.distributions.Dirichlet(torch.ones(vocab_size) * alpha)
    return dist.sample((batch_size, seq_len))
```

#### Step 2.2: Flow Matching Loss

```python
def flow_matching_loss(
    model: nn.Module,
    x1: torch.Tensor,      # [B, L, vocab] one-hot target
    cond: Dict,            # æ¡ä»¶ä¿¡æ¯
    alpha: float = 1.0,    # Dirichlet å‚æ•°
) -> torch.Tensor:
    B, L, V = x1.shape
    device = x1.device
    
    # 1. é‡‡æ · x0 (å…ˆéªŒ)
    x0 = sample_x0_dirichlet(B, L, V, alpha).to(device)
    
    # 2. é‡‡æ · t ~ Uniform(0, 1)
    t = torch.rand(B, 1, 1, device=device)
    
    # 3. è®¡ç®— x_t
    x_t = dirichlet_interpolate(x0, x1, t)
    
    # 4. ç›®æ ‡é€Ÿåº¦åœº v* = x1 - x0
    v_target = x1 - x0
    
    # 5. æ¨¡å‹é¢„æµ‹é€Ÿåº¦åœº
    v_pred = model(x_t, t.squeeze(-1), cond)
    
    # 6. MSE loss
    loss = F.mse_loss(v_pred, v_target)
    
    return loss
```

#### Step 2.3: å®Œæ•´è®­ç»ƒå¾ªç¯

```python
def train_epoch(model, encoder, loader, optimizer, cfg_drop_prob=0.1):
    model.train()
    encoder.train()
    total_loss = 0
    
    for batch in loader:
        # 1. ç¼–ç æ¡ä»¶
        cond = encoder(
            cdr3_xt=None,  # è®­ç»ƒæ—¶ä¸éœ€è¦
            peptide=batch['peptide'],
            mhc=batch['mhc'],
            scaffold_seqs=batch['scaffold_seqs'],
            conditioning_info=['pep', 'mhc', 'hv', 'hj', 'lv', 'lj'],
        )
        
        # 2. CFG: éšæœº drop æ¡ä»¶
        if torch.rand(1).item() < cfg_drop_prob:
            cond = None  # æˆ–ç”¨ learned uncond embedding
        
        # 3. å‡†å¤‡ target (one-hot CDR3Î²)
        x1 = F.one_hot(batch['cdr3b_tokens'], num_classes=model.vocab_size).float()
        
        # 4. Flow matching loss
        loss_flow = flow_matching_loss(model, x1, cond)
        
        # 5. (å¯é€‰) Collapse entropy æ­£åˆ™
        loss_entropy = compute_collapse_entropy(encoder, batch)
        
        # 6. æ€» loss
        loss = loss_flow + Î»_ent * loss_entropy
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(loader)
```

---

### Phase 3: CFG å®ç° (Day 7-8)

#### Step 3.1: è®­ç»ƒæ—¶ Condition Drop

```python
class CFGWrapper(nn.Module):
    """
    Classifier-Free Guidance åŒ…è£…å™¨
    """
    def __init__(self, model, drop_prob=0.1):
        super().__init__()
        self.model = model
        self.drop_prob = drop_prob
        # å¯å­¦ä¹ çš„ unconditional embedding
        self.uncond_emb = nn.Parameter(torch.zeros(1, model.hidden_dim))
    
    def forward(self, x_t, t, cond, training=True):
        if training and torch.rand(1).item() < self.drop_prob:
            # Drop condition â†’ ä½¿ç”¨ uncond embedding
            cond = self.uncond_emb.expand(x_t.size(0), -1)
        return self.model(x_t, t, cond)
```

#### Step 3.2: æ¨ç†æ—¶ CFG

```python
def sample_with_cfg(
    model: nn.Module,
    cond: torch.Tensor,
    uncond: torch.Tensor,
    seq_len: int,
    n_steps: int = 100,
    cfg_weight: float = 1.5,
) -> torch.Tensor:
    """
    CFG é‡‡æ ·:
    v_final = v_uncond + w * (v_cond - v_uncond)
    """
    device = cond.device
    B = cond.size(0)
    
    # åˆå§‹åŒ– x_0 (uniform)
    x = torch.ones(B, seq_len, model.vocab_size, device=device) / model.vocab_size
    
    dt = 1.0 / n_steps
    
    for step in range(n_steps):
        t = torch.full((B, 1), step / n_steps, device=device)
        
        # æœ‰æ¡ä»¶é¢„æµ‹
        v_cond = model(x, t, cond)
        
        # æ— æ¡ä»¶é¢„æµ‹
        v_uncond = model(x, t, uncond)
        
        # CFG ç»„åˆ
        v = v_uncond + cfg_weight * (v_cond - v_uncond)
        
        # Euler step
        x = x + v * dt
        
        # æŠ•å½±å› simplex (å½’ä¸€åŒ–)
        x = F.softmax(x, dim=-1)
    
    # æœ€ç»ˆè§£ç 
    tokens = x.argmax(dim=-1)
    return tokens
```

---

### Phase 4: Model Score Hook (Day 9)

#### Step 4.1: å®šä¹‰ Model Score

```python
def compute_model_score(model, encoder, cdr3_tokens, cond):
    """
    è®¡ç®—ç”Ÿæˆåºåˆ—çš„ model scoreï¼Œç”¨äº hybrid MC energy
    
    å¯é€‰å®šä¹‰:
    1. Flow cost: ç§¯åˆ† ||v_Î¸(x_t, t)||Â² dt
    2. Collapse scalar: Ïˆ token çš„æŸä¸ªæŠ•å½±
    3. Approximate NLL
    """
    # æ–¹æ³• 1: è¿‘ä¼¼ NLL (é€šè¿‡ ODE likelihood)
    x1 = F.one_hot(cdr3_tokens, model.vocab_size).float()
    
    # åå‘ ODE è®¡ç®— log_prob
    log_prob = compute_ode_log_prob(model, x1, cond)
    
    return -log_prob  # è´Ÿ log prob ä½œä¸º score (è¶Šä½è¶Šå¥½)
```

#### Step 4.2: å¯¼å‡º Hook

```python
class FlowTCRGen(nn.Module):
    def __init__(self, ...):
        ...
    
    def get_model_score(self, cdr3_seq: str, cond: Dict) -> float:
        """
        ä¾› Stage 3 MC ä½¿ç”¨çš„æ¥å£
        """
        tokens = self.tokenize(cdr3_seq)
        with torch.no_grad():
            score = compute_model_score(self.model, self.encoder, tokens, cond)
        return score.item()
```

---

### Phase 5: è¯„ä¼°æŒ‡æ ‡ (Day 10)

#### Step 5.1: Recovery Rate

```python
def evaluate_recovery(model, val_loader, n_samples=10):
    """
    è®¡ç®—ç”Ÿæˆçš„ CDR3Î² ä¸çœŸå®åºåˆ—çš„åŒ¹é…ç‡
    """
    exact_match = 0
    total = 0
    
    for batch in val_loader:
        cond = encode_condition(batch)
        
        for _ in range(n_samples):
            generated = model.sample(cond)
            for i, (gen, gt) in enumerate(zip(generated, batch['cdr3b'])):
                if gen == gt:
                    exact_match += 1
                total += 1
    
    return exact_match / total
```

#### Step 5.2: Diversity

```python
def evaluate_diversity(model, val_loader, n_samples=100):
    """
    è®¡ç®—ç”Ÿæˆåºåˆ—çš„å¤šæ ·æ€§ (unique ratio)
    """
    all_generated = set()
    
    for batch in val_loader:
        cond = encode_condition(batch)
        for _ in range(n_samples):
            generated = model.sample(cond)
            all_generated.update(generated)
    
    return len(all_generated) / (len(val_loader) * n_samples)
```

#### Step 5.3: Perplexity

```python
def evaluate_perplexity(model, val_loader):
    """
    è®¡ç®—éªŒè¯é›†ä¸Šçš„å›°æƒ‘åº¦
    """
    total_nll = 0
    total_tokens = 0
    
    for batch in val_loader:
        cond = encode_condition(batch)
        x1 = F.one_hot(batch['cdr3b_tokens'], model.vocab_size).float()
        
        # è®¡ç®— NLL
        nll = compute_ode_log_prob(model, x1, cond)
        total_nll += nll.sum().item()
        total_tokens += x1.size(0) * x1.size(1)
    
    return torch.exp(torch.tensor(total_nll / total_tokens))
```

---

## 4. Reminders âš ï¸

### 4.1 è®­ç»ƒé…ç½®
- **CFG drop prob**: 0.1ï¼ˆè®­ç»ƒæ—¶ 10% æ¦‚ç‡ drop æ¡ä»¶ï¼‰
- **CFG weight**: 1.0-2.0ï¼ˆæ¨ç†æ—¶å¯è°ƒï¼‰
- **Î»_ent**: 0.01ï¼ˆcollapse entropy æ­£åˆ™æƒé‡ï¼‰
- **Î»_prof**: 0.01ï¼ˆprofile æ­£åˆ™æƒé‡ï¼‰
- **ODE steps**: 100ï¼ˆé‡‡æ ·æ­¥æ•°ï¼‰

### 4.2 é•¿åºåˆ—å¤„ç†
- **MHC åºåˆ—**ï¼šå¯èƒ½å¾ˆé•¿ï¼ˆ>200aaï¼‰ï¼Œéœ€è¦æˆªæ–­æˆ– chunked attention
- **æ‹¼æ¥é¡ºåº**ï¼š`[Ïˆ, CDR3Î², peptide, MHC, HV, HJ, LV, LJ]`
- **ä½ç½®ç¼–ç **ï¼šæ¯ä¸ªåŒºåŸŸæœ‰ç‹¬ç«‹çš„ä½ç½® offset

### 4.3 ä¸ Stage 1 æ¥å£
- **è¾“å…¥**ï¼šéœ€è¦ Stage 1 çš„ `encode_pmhc()` å’Œ `retrieve_scaffolds()`
- **æ¡ä»¶æ ¼å¼**ï¼šç¡®ä¿ embedding ç»´åº¦åŒ¹é…

### 4.4 ä»£ç é£æ ¼
- **å¤ç”¨ psi_model**ï¼šä¸è¦ copy-pasteï¼Œç›´æ¥ import
- **Checkpoint è·¯å¾„**ï¼šä¿å­˜åˆ° `checkpoints/stage2_v1/`
- **æ—¥å¿—**ï¼šæ¯ epoch æ‰“å° loss åˆ†è§£ã€recoveryã€diversity

---

## 5. Checklist

### Phase 1: å¤ç”¨ psi_model
- [ ] ç¡®è®¤ `psi_model/model.py` å¯ import
- [ ] åˆ›å»º `FlowTCRGenEncoder` é€‚é…å™¨ç±»
- [ ] å®ç° x_t æ³¨å…¥æ–¹å¼
- [ ] æµ‹è¯• `create_hierarchical_pairs()` è¾“å‡ºæ­£ç¡®

### Phase 2: Dirichlet Flow
- [ ] å®ç° `sample_x0_dirichlet()`
- [ ] å®ç° `dirichlet_interpolate()`
- [ ] å®ç° `flow_matching_loss()`
- [ ] ä¿®æ”¹ `train_flow.py` ä½¿ç”¨æ–° loss

### Phase 3: CFG
- [ ] å®ç° `CFGWrapper` ç±»
- [ ] è®­ç»ƒæ—¶ condition drop
- [ ] å®ç° `sample_with_cfg()`
- [ ] æ·»åŠ  `--cfg_weight` å‘½ä»¤è¡Œå‚æ•°

### Phase 4: Model Score Hook
- [ ] å®šä¹‰ `compute_model_score()` å‡½æ•°
- [ ] åœ¨ `FlowTCRGen` ç±»ä¸­å¯¼å‡º `get_model_score()` æ¥å£
- [ ] æµ‹è¯•ä¸ Stage 3 MC çš„é›†æˆ

### Phase 5: è¯„ä¼°æŒ‡æ ‡
- [ ] å®ç° `evaluate_recovery()`
- [ ] å®ç° `evaluate_diversity()`
- [ ] å®ç° `evaluate_perplexity()`
- [ ] åœ¨éªŒè¯å¾ªç¯ä¸­è°ƒç”¨

### Phase 6: Ablation Studies (å¿…åš)
- [ ] æ·»åŠ  `--use_collapse` å‚æ•°å’Œå¼€å…³
- [ ] æ·»åŠ  `--use_hier_pairs` å‚æ•°å’Œå¼€å…³
- [ ] å®ç° CFG weight sweep è„šæœ¬
- [ ] å®ç° conditioning components ablation
- [ ] ç”Ÿæˆ Ablation ç»“æœè¡¨æ ¼

### Phase 6: Ablation Studies (å¿…åš)

#### Step 6.1: Collapse Token Ablation

**ç›®æ ‡**ï¼šéªŒè¯ Collapse Token (Ïˆ) çš„è´¡çŒ®ï¼ˆè®ºæ–‡æ ¸å¿ƒ claimï¼‰

```python
# é…ç½®æ¥å£
ablation_configs = [
    {'name': 'with_collapse', 'use_collapse': True},   # é»˜è®¤
    {'name': 'no_collapse', 'use_collapse': False},    # å»æ‰ Ïˆ token
]

# åœ¨ FlowTCRGenEncoder ä¸­æ·»åŠ å¼€å…³
class FlowTCRGenEncoder(nn.Module):
    def __init__(self, ..., use_collapse: bool = True):
        self.use_collapse = use_collapse
        if use_collapse:
            self.collapse_token = nn.Parameter(torch.randn(1, 1, s_dim))
```

**é¢„æœŸç»“æœ**ï¼šwith_collapse çš„ recovery/diversity åº”æ˜¾è‘—é«˜äº no_collapse

#### Step 6.2: Hierarchical Pairs Ablation

**ç›®æ ‡**ï¼šéªŒè¯ 7-level æ‹“æ‰‘ç¼–ç çš„è´¡çŒ®ï¼ˆè®ºæ–‡æ ¸å¿ƒ claimï¼‰

```python
ablation_configs = [
    {'name': 'hier_pairs', 'use_hier_pairs': True},      # é»˜è®¤
    {'name': 'flat_pairs', 'use_hier_pairs': False},     # æ‰€æœ‰ pair åŒ level
]

# åœ¨ create_hierarchical_pairs ä¸­æ·»åŠ å¼€å…³
def create_hierarchical_pairs(..., use_hier: bool = True):
    if not use_hier:
        return torch.zeros(L, L, dtype=torch.long)  # å…¨éƒ¨ level=0
    # æ­£å¸¸ 7-level é€»è¾‘
```

#### Step 6.3: CFG Ablation

**ç›®æ ‡**ï¼šéªŒè¯ CFG å¯¹ç”Ÿæˆè´¨é‡çš„å½±å“

```python
ablation_configs = [
    {'name': 'cfg_1.0', 'cfg_weight': 1.0},
    {'name': 'cfg_1.5', 'cfg_weight': 1.5},
    {'name': 'cfg_2.0', 'cfg_weight': 2.0},
    {'name': 'no_cfg', 'cfg_weight': 0.0},  # çº¯æ— æ¡ä»¶
]
```

#### Step 6.4: Conditioning Components Ablation

**ç›®æ ‡**ï¼šéªŒè¯å„æ¡ä»¶ç»„ä»¶çš„è´¡çŒ®

```python
# é€šè¿‡ conditioning_info æ§åˆ¶
ablation_configs = [
    {'name': 'full', 'cond': ['pep', 'mhc', 'hv', 'hj', 'lv', 'lj']},
    {'name': 'no_scaffold', 'cond': ['pep', 'mhc']},
    {'name': 'no_peptide', 'cond': ['mhc', 'hv', 'hj', 'lv', 'lj']},
    {'name': 'scaffold_only', 'cond': ['hv', 'hj', 'lv', 'lj']},
]
```

---

### Phase 7: é›†æˆæµ‹è¯•
- [ ] ç«¯åˆ°ç«¯è®­ç»ƒ 100 epochs
- [ ] éªŒè¯ recovery > 30%
- [ ] éªŒè¯ PPL < 10
- [ ] ä¿å­˜æœ€ä½³ checkpoint

---

## 6. Ablation Checklist (å¿…åš)

| Ablation | é…ç½® | æŒ‡æ ‡ | çŠ¶æ€ |
|----------|------|------|------|
| Â±Collapse Token | `use_collapse = T/F` | Recovery, Diversity | [ ] |
| Â±Hierarchical Pairs | `use_hier_pairs = T/F` | Recovery, Diversity | [ ] |
| CFG weight sweep | `cfg_weight = {0, 1.0, 1.5, 2.0}` | Recovery vs Diversity trade-off | [ ] |
| Conditioning components | è§ Step 6.4 | Recovery | [ ] |

---

## 7. Exploratory (å¾…åšäº‹é¡¹)

> ä»¥ä¸‹ä¸ºå¯é€‰æ¢ç´¢é¡¹ï¼Œä¸é˜»å¡ä¸»çº¿ï¼Œä½†ä¿ç•™æ¥å£ä»¥ä¾¿åç»­å¼€å‘ã€‚

### ğŸŸ¢ E1: Physics Gradient Guidance in ODE
- **ç›®æ ‡**ï¼šåœ¨ ODE é‡‡æ ·ä¸­æ³¨å…¥ âˆ‡E_Ï† æ¢¯åº¦
- **å…¬å¼**ï¼š`x_{t+Î”t} = x_t + (v_Î¸ - wâˆ‡E_Ï†)Î”t`
- **æ¥å£é¢„ç•™**ï¼š`sample_with_cfg(..., energy_model=None, energy_weight=0.0)`
- **ä¾èµ–**ï¼šStage 3 E_Ï† å®Œæˆ
- **çŠ¶æ€**ï¼š[ ] å¾…å®ç°

### ğŸŸ¢ E2: Entropy Scheduling
- **ç›®æ ‡**ï¼šåœ¨ ODE ä¸åŒé˜¶æ®µä½¿ç”¨ä¸åŒçš„ entropy æ­£åˆ™
- **æ–¹æ¡ˆ**ï¼šæ—©æœŸé«˜ entropyï¼ˆæ¢ç´¢ï¼‰ï¼ŒåæœŸä½ entropyï¼ˆæ”¶æ•›ï¼‰
- **æ¥å£é¢„ç•™**ï¼š`EntropyScheduler` ç±»
- **çŠ¶æ€**ï¼š[ ] å¾…å®ç°

### ğŸŸ¢ E3: Multi-CDR Generation
- **ç›®æ ‡**ï¼šåŒæ—¶ç”Ÿæˆ CDR3Î± å’Œ CDR3Î²
- **æ–¹æ¡ˆ**ï¼šæ‰©å±• HD åŒºåŸŸåŒ…å«åŒé“¾
- **æ¥å£é¢„ç•™**ï¼š`generate(..., targets=['cdr3a', 'cdr3b'])`
- **çŠ¶æ€**ï¼š[ ] å¾…è®¾è®¡

### ğŸŸ¢ E4: Self-Play with Stage 3 Feedback
- **ç›®æ ‡**ï¼šç”¨ Stage 3 E_Ï† è¯„åˆ†åé¦ˆè®­ç»ƒ Stage 2
- **æ–¹æ¡ˆ**ï¼šå¯¹é«˜åˆ†ç”Ÿæˆç»“æœå¢åŠ è®­ç»ƒæƒé‡
- **æ¥å£é¢„ç•™**ï¼š`update_with_energy_feedback(generated, scores)`
- **çŠ¶æ€**ï¼š[ ] å¾…è®¾è®¡

---

## 8. æˆåŠŸæ ‡å‡†

| æŒ‡æ ‡ | ç›®æ ‡ |
|------|------|
| Recovery Rate | **> 30%** |
| Diversity | **> 50%** unique in 100 samples |
| Perplexity | **< 10** |
| è®­ç»ƒæ—¶é—´ | < 48h @1Ã—A100 |
| Ablation: Â±collapse delta | è®°å½•æ˜¾è‘—å·®å¼‚ |
| Ablation: Â±hier_pairs delta | è®°å½•æ˜¾è‘—å·®å¼‚ |

---

## 9. ä¸å…¶ä»– Stage çš„æ¥å£

### è¾“å…¥æ¥è‡ª Stage 1 (Immuno-PLM)

```python
# ä» Stage 1 è·å– scaffold
from flowtcr_fold.Immuno_PLM import ImmunoPLM

plm = ImmunoPLM.load("checkpoints/stage1_v1/best.pt")
scaffolds = plm.retrieve_scaffolds(pmhc_emb, top_k=10)
```

### è¾“å‡ºç»™ Stage 3 (TCRFold-Prophet)

```python
# Stage 2 æä¾›çš„ API
class FlowTCRGen:
    def sample(self, cond: Dict, n_samples: int = 100) -> List[str]:
        """ç”Ÿæˆ CDR3Î² åºåˆ—"""
        pass
    
    def get_model_score(self, cdr3_seq: str, cond: Dict) -> float:
        """è¿”å› model score ç”¨äº hybrid MC"""
        pass
```

---

**Last Updated**: 2025-12-01  
**Owner**: Stage 2 Implementation Team

