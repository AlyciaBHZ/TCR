# FlowTCR-Fold: Physics-Grounded Generative TCR Design

> **Two-Stage TCR Design Framework**: Scaffold Retrieval + CDR3Î² Generation with Flow Matching, Topology Priors, and Physics Guidance.

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Core Design Philosophy](#2-core-design-philosophy)
3. [Two-Stage Design Strategy](#3-two-stage-design-strategy)
4. [Module Architecture](#4-module-architecture)
5. [Data Infrastructure](#5-data-infrastructure)
6. [Training Workflows](#6-training-workflows)
7. [Inference Pipeline](#7-inference-pipeline)
8. [Code Layout](#8-code-layout)
9. [Quickstart Guide](#9-quickstart-guide)
10. [Legacy Code References](#10-legacy-code-references)
11. [Status & Roadmap](#11-status--roadmap)

---

## 1. Project Overview

### 1.1 Scientific Goal

Design **antigen-specific T Cell Receptors (TCRs)** given a target peptide-MHC (pMHC) complex. This has transformative implications for:
- Cancer immunotherapy (CAR-T, TCR-T)
- Vaccine development
- Autoimmune disease treatment

### 1.2 Technical Challenge

TCR-pMHC recognition is governed by complex sequence-structure interactions:
- **CDR3Î² loop**: Primary determinant of antigen specificity
- **V/J gene scaffolds**: Provide structural framework and MHC compatibility
- **Multi-chain topology**: TCRÎ±/Î² chains interact with pMHC in a coordinated manner

### 1.3 Our Approach

A **Retrieve & Generate** framework that decomposes the problem into two tractable sub-tasks:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: Target pMHC (peptide + MHC allele)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: SCAFFOLD RETRIEVAL                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  Immuno-PLM retrieves Top-K V/J scaffolds (lv, lj, hv, hj)     â”‚
â”‚  that are compatible with the target MHC                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: CDR3Î² GENERATION                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  FlowTCR-Gen generates CDR3Î² conditioned on pMHC + scaffold    â”‚
â”‚  using Discrete Flow Matching                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: STRUCTURE CRITIQUE (Optional)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  TCRFold-Light + EvoEF2 filter and rank candidates             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Complete TCR sequence (scaffold + CDR3Î²)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Core Design Philosophy

### 2.1 Hybrid Intelligence

We do not rely solely on data fitting. The system combines:

| Component | Role | Source |
|-----------|------|--------|
| **Discrete Flow Matching** | Global sequence search | FlowTCR-Gen |
| **ESM-2 / Topology Embeddings** | Universal protein semantics | Immuno-PLM |
| **EvoEF2 Energy** | First-principles physics | Physics module |
| **Hierarchical Pair Bias** | TCR-pMHC topology awareness | Legacy psi_model |

### 2.2 Retrieve & Generate Paradigm

**Why not generate scaffolds directly?**

| Challenge | Explanation |
|-----------|-------------|
| Discrete space | V/J genes are categorical (e.g., TRBV19*01), not continuous |
| Combinatorial explosion | VÎ² Ã— JÎ² Ã— VÎ± Ã— JÎ± = hundreds of thousands of combinations |
| Data sparsity | Many combinations appear only a few times in training data |

**Solution**: Retrieve scaffolds from a pre-computed bank, then generate CDR3Î².

### 2.3 Safe Contrastive Learning

To avoid "false negative" issues in InfoNCE training:

- **Batch Random Negatives**: Use other samples in the same batch as negatives
- **No explicit hard negative mining from database**: Avoids accidentally marking true binders as negatives
- **Synthetic negatives (optional)**: Mutate anchor positions to create guaranteed non-binders

---

## 3. Two-Stage Design Strategy

### 3.1 Stage 1: Scaffold Retrieval

**Objective**: Find V/J gene combinations compatible with the target MHC.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCAFFOLD RETRIEVAL                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input:  Target pMHC sequence                                â”‚
â”‚                                                              â”‚
â”‚  Model:  Immuno-PLM (InfoNCE encoder)                        â”‚
â”‚                                                              â”‚
â”‚  Bank:   Pre-computed scaffold embeddings                    â”‚
â”‚          - Key: (h_v, h_j, l_v, l_j) gene combination        â”‚
â”‚          - Value: Germline amino acid sequences              â”‚
â”‚          - Vector: Immuno-PLM embeddings                     â”‚
â”‚                                                              â”‚
â”‚  Method:                                                     â”‚
â”‚    1. Encode pMHC with Immuno-PLM                            â”‚
â”‚    2. Compute cosine similarity with scaffold bank           â”‚
â”‚    3. Retrieve Top-K scaffolds                               â”‚
â”‚                                                              â”‚
â”‚  Output: Top-K scaffold sequences                            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Physical Interpretation**: Select V genes whose structural framework is compatible with the target MHC's binding groove.

### 3.2 Stage 2: CDR3Î² Generation

**Objective**: Generate CDR3Î² loop that binds the target peptide.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CDR3Î² GENERATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input:  pMHC + Scaffold (from Stage 1)                      â”‚
â”‚                                                              â”‚
â”‚  Model:  FlowTCR-Gen (Conditional Flow Matching)             â”‚
â”‚                                                              â”‚
â”‚  Conditioning:                                               â”‚
â”‚    - pMHC embedding (from Immuno-PLM)                        â”‚
â”‚    - Scaffold embedding (from Immuno-PLM)                    â”‚
â”‚    - (Optional) TM-align PSSM for structural prior           â”‚
â”‚    - (Optional) Geometry summary from TCRFold-Light          â”‚
â”‚                                                              â”‚
â”‚  Method:                                                     â”‚
â”‚    - Dirichlet Flow Matching on amino acid simplex           â”‚
â”‚    - Vector field prediction: v_Î¸(x_t, t, cond)              â”‚
â”‚    - Loss: ||v_Î¸ - (y - x_0)||Â²                              â”‚
â”‚                                                              â”‚
â”‚  Output: CDR3Î² sequence candidates                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Stage 3: Structure Critique (Optional)

**Objective**: Filter structurally implausible candidates.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STRUCTURE CRITIQUE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input:  CDR3Î² candidates + Scaffold                         â”‚
â”‚                                                              â”‚
â”‚  Model:  TCRFold-Light (MSA-free Evoformer)                  â”‚
â”‚                                                              â”‚
â”‚  Outputs:                                                    â”‚
â”‚    - Contact map prediction                                  â”‚
â”‚    - pLDDT-like confidence score                             â”‚
â”‚    - Energy surrogate (trained on EvoEF2 labels)             â”‚
â”‚                                                              â”‚
â”‚  Filtering:                                                  â”‚
â”‚    - Remove candidates with low interface contact density    â”‚
â”‚    - Remove candidates with high predicted energy            â”‚
â”‚                                                              â”‚
â”‚  (Optional) EvoEF2 Refinement:                               â”‚
â”‚    - Monte Carlo sidechain repacking                         â”‚
â”‚    - Compute precise binding energy (Î”Î”G)                    â”‚
â”‚    - Final ranking                                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Module Architecture

### 4.1 Immuno-PLM (ESM-2 + Topology Bias) â€” Status: **partial**

**Role**: Encode TCR and pMHC sequences into embeddings for retrieval and conditioning.

**Core Design**: Topology bias + V/J conditioning. Current code supports BasicTokenizer and optional ESM with in-house LoRA (no  dependency).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Immuno-PLM Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Tokens                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚      ESM-2 (esm2_t33_650M_UR50D) + LoRA Adapters          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚  Each Self-Attention Layer:                         â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  Q_proj + LoRA | K_proj + LoRA | V_proj + LoRA     â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚  Ã— 33 layers                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚               Sequence Features [B, L, 1280]                    â”‚
â”‚                             â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚                             â”‚                    â”‚
â”‚              â–¼                             â–¼                    â”‚
â”‚      seq_proj [1280â†’256]        TopologyBias (from psi_model)  â”‚
â”‚              â”‚                   - 7-level hierarchy            â”‚
â”‚              â”‚                   - pair_embed_lvl1/2            â”‚
â”‚              â”‚                             â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º + â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚                 Fused Features [B, L, 256]                      â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚                 Masked Pooling + LayerNorm                      â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚                 Pooled [B, 256] â”€â”€â–º contrastive_head            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

_LoRA adapters are part of the design but are not implemented in the current codebase; current backbone is BasicTokenizer or frozen ESM features._

**Training (implemented subset)**:
`ash
python flowtcr_fold/Immuno_PLM/train_plm.py --data data/trn.csv --batch_size 32 --epochs 100
# Optional (frozen ESM features if installed):
python flowtcr_fold/Immuno_PLM/train_plm.py --data data/trn.csv --use_esm --esm_model esm2_t6_8M_UR50D
`

**Loss**: Batch InfoNCE (safe from false negatives)

### 4.2 FlowTCR-Gen (Flow Matching Generator)

**Role**: Generate CDR3Î² sequences given pMHC and scaffold conditions.

```python
class FlowMatchingModel(nn.Module):
    """
    Discrete Flow Matching for CDR3Î² generation.
    
    Flow setup:
    - Base x_0: Uniform distribution over amino acids
    - Target y: One-hot ground truth sequence
    - Interpolant: x_t = (1-t) * x_0 + t * y
    - Vector field: v* = y - x_0
    - Loss: ||v_Î¸(x_t, t, cond) - v*||Â²
    """
    
    def __init__(self, vocab_size=21, hidden_dim=256, n_layers=6):
        # Conditioning encoder
        # Time embedding
        # Transformer backbone
        # Vector field head
        pass
    
    def forward(self, x_t, t, condition):
        # Returns: predicted vector field [B, L, vocab_size]
        pass
```

**Conditioning Inputs**:
1. pMHC embedding (from Immuno-PLM)
2. Scaffold embedding (from Immuno-PLM)
3. (Optional) TM-align PSSM
4. (Optional) Geometry features from TCRFold-Light

### 4.3 TCRFold-Light (Structure Critic)

**Role**: Predict structural features and energy for candidate filtering.

```python
class TCRFoldLight(nn.Module):
    """
    MSA-free Evoformer-lite for structure prediction.
    
    Outputs:
    - Distance map: [B, L, L, n_bins]
    - Contact map: [B, L, L, 1]
    - Energy: [B, 1] (surrogate for EvoEF2)
    """
    
    def __init__(self, s_dim=512, z_dim=128, n_layers=12):
        # Evoformer blocks (Triangle updates + attention)
        # Distance head
        # Contact head
        # Energy head
        pass
```

**Training**:
- **Phase 1**: Generic PPI pretraining (PDB contacts)
- **Phase 2**: TCR-specific finetuning (STCRDab/TCR3d)
- **Losses**: Distance MSE + Contact BCE (10Ã— weight for interface) + Energy MSE

---

## 5. Data Infrastructure

### 5.1 Data Sources

| Dataset | Size | Fields | Usage |
|---------|------|--------|-------|
| **Paired TCR-pMHC** (trn.csv) | 200K+ | peptide, mhc, cdr3_b, h_v, h_j, l_v, l_j | Scaffold bank, Immuno-PLM, FlowTCR-Gen |
| **TCRdb CDR3Î²** | Large | cdr3_b only | (Optional) Flow pretraining |
| **STCRDab / TCR3d** | ~500 | PDB structures | TCRFold-Light training |

### 5.2 Data Fields

```
Required:
â”œâ”€â”€ peptide    : Antigenic peptide sequence (8-15 aa)
â”œâ”€â”€ mhc        : MHC allele name or sequence
â””â”€â”€ cdr3_b     : CDR3Î² sequence (target for generation)

Optional:
â”œâ”€â”€ h_v        : Heavy chain V gene
â”œâ”€â”€ h_j        : Heavy chain J gene
â”œâ”€â”€ l_v        : Light chain V gene (alpha)
â”œâ”€â”€ l_j        : Light chain J gene (alpha)
â””â”€â”€ cdr3_a     : CDR3Î± sequence
```

### 5.3 Scaffold Bank Construction

```python
# Extract unique V/J combinations from paired data
import pandas as pd

df = pd.read_csv("data/trn.csv")

# Group by V/J genes
scaffold_bank = df.groupby(['h_v', 'h_j', 'l_v', 'l_j']).agg({
    'peptide': 'first',  # Representative peptide
    'mhc': 'first',      # Representative MHC
    'cdr3_b': 'count'    # Frequency
}).reset_index()

scaffold_bank.columns = ['h_v', 'h_j', 'l_v', 'l_j', 'rep_peptide', 'rep_mhc', 'count']
scaffold_bank.to_csv("data/scaffold_bank.csv", index=False)

print(f"Unique scaffolds: {len(scaffold_bank)}")
```

### 5.4 Hard Negative Strategies

| Type | Strategy | Safety |
|------|----------|--------|
| **Batch Random** | Other samples in batch as negatives | âœ… Safe |
| **Peptide Decoy** | Same MHC, similar peptide (60-90% identity) | âš ï¸ Moderate |
| **CDR3 Mutant** | Same pMHC, 2-3 point mutations in CDR3 | âš ï¸ Moderate |
| **Synthetic** | Mutate anchor positions to opposite charge | âœ… Safe |

---

## 6. Training Workflows

### 6.1 Immuno-PLM Training

**Objective**: Learn TCR-pMHC compatibility for scaffold retrieval.

```bash
python flowtcr_fold/Immuno_PLM/train_plm.py \
    --data data/trn.csv \
    --epochs 100 \
    --batch_size 64 \
    --lr 1e-4 \
    --tau 0.07 \
    --out_dir checkpoints/plm
```

**Loss Function** (Batch InfoNCE):

```python
def compute_batch_infonce(tcr_emb, pmhc_emb, temperature=0.07):
    # tcr_emb: [B, D], pmhc_emb: [B, D]
    logits = tcr_emb @ pmhc_emb.T / temperature  # [B, B]
    labels = torch.arange(logits.size(0), device=logits.device)
    return F.cross_entropy(logits, labels)
```

### 6.2 FlowTCR-Gen Training

**Objective**: Learn to generate CDR3Î² given pMHC + scaffold.

```bash
python flowtcr_fold/FlowTCR_Gen/train_flow.py \
    --data data/trn.csv \
    --epochs 100 \
    --batch_size 32 \
    --lr 1e-4 \
    --out_dir checkpoints/flow
```

**Loss Function** (Flow Matching):

```python
def flow_matching_loss(model, x_0, y, condition):
    # x_0: uniform noise [B, L, vocab]
    # y: one-hot target [B, L, vocab]
    t = torch.rand(x_0.size(0), 1, 1, device=x_0.device)
    x_t = (1 - t) * x_0 + t * y  # Interpolant
    v_target = y - x_0           # Target vector field
    v_pred = model(x_t, t, condition)
    return F.mse_loss(v_pred, v_target)
```

### 6.3 TCRFold-Light Training

**Objective**: Learn structure prediction with energy supervision.

```bash
python flowtcr_fold/TCRFold_Light/train_with_energy.py \
    --pdb_dir data/pdb_structures \
    --cache_dir data/energy_cache \
    --epochs 100 \
    --batch_size 4 \
    --interface_weight 10.0 \
    --out_dir checkpoints/tcrfold
```

**Loss Function** (Physics-guided):

```python
def compute_physics_loss(pred, target, interface_mask):
    # Distance loss
    loss_dist = F.mse_loss(pred['distance'], target['distance'])
    
    # Contact loss (10Ã— weight for interface)
    loss_contact = F.binary_cross_entropy(
        pred['contact'], target['contact'],
        weight=1 + 9 * interface_mask  # 10Ã— for interface
    )
    
    # Energy loss (EvoEF2 supervision)
    loss_energy = F.mse_loss(pred['energy'], target['energy'])
    
    return loss_dist + loss_contact + loss_energy
```

### 6.4 Training Preferences

| Setting | Value | Location |
|---------|-------|----------|
| Checkpoint frequency | Every 50 epochs | `common/utils.py` |
| Early stopping patience | 100 epochs | `common/utils.py` |
| Gradient clipping | max_norm=1.0 | Training scripts |

---

## 7. Inference Pipeline

### 7.1 Complete Workflow

```python
from flowtcr_fold.Immuno_PLM import ImmunoPLM
from flowtcr_fold.FlowTCR_Gen import FlowMatchingModel
from flowtcr_fold.TCRFold_Light import TCRFoldLight
from flowtcr_fold.physics import TCRStructureOptimizer

# 1. Load models
plm = ImmunoPLM.load("checkpoints/plm/immuno_plm.pt")
flow = FlowMatchingModel.load("checkpoints/flow/flow_gen.pt")
critic = TCRFoldLight.load("checkpoints/tcrfold/tcrfold_light.pt")
optimizer = TCRStructureOptimizer()

# 2. Encode target pMHC
target_pmhc = {"peptide": "GILGFVFTL", "mhc": "HLA-A*02:01"}
pmhc_emb = plm.encode_pmhc(target_pmhc)

# 3. Stage 1: Retrieve Top-K scaffolds
scaffold_bank = load_scaffold_bank("data/scaffold_bank.csv")
scaffold_embs = plm.encode_scaffolds(scaffold_bank)
similarities = pmhc_emb @ scaffold_embs.T
top_k_indices = similarities.topk(10).indices
top_scaffolds = [scaffold_bank[i] for i in top_k_indices]

# 4. Stage 2: Generate CDR3Î² for each scaffold
candidates = []
for scaffold in top_scaffolds:
    scaffold_emb = plm.encode_scaffold(scaffold)
    condition = torch.cat([pmhc_emb, scaffold_emb], dim=-1)
    
    # Sample multiple CDR3Î² sequences
    for _ in range(100):
        cdr3b = flow.sample(condition)
        candidates.append({
            "scaffold": scaffold,
            "cdr3b": cdr3b,
            "condition": condition
        })

# 5. Stage 3: Critique and rank
scored_candidates = []
for cand in candidates:
    # TCRFold-Light scoring
    score = critic.score(cand["scaffold"], cand["cdr3b"])
    cand["tcrfold_score"] = score
    
    # (Optional) EvoEF2 refinement
    if score > threshold:
        energy = optimizer.compute_binding_energy(cand)
        cand["energy"] = energy
    
    scored_candidates.append(cand)

# 6. Final ranking
scored_candidates.sort(key=lambda x: x.get("energy", x["tcrfold_score"]))
top_designs = scored_candidates[:10]
```

### 7.2 Command-Line Interface

```bash
# Run full pipeline
python flowtcr_fold/FlowTCR_Gen/pipeline_impl.py \
    --peptide "GILGFVFTL" \
    --mhc "HLA-A*02:01" \
    --top_k_scaffolds 10 \
    --samples_per_scaffold 100 \
    --output results/designs.csv
```

---

## 8. Code Layout

```
flowtcr_fold/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ TODO.md                             # Task tracking
â”œâ”€â”€ EVOEF2_INTEGRATION.md               # EvoEF2 setup guide
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ USER_MANUAL.md                  # User guide (ä¸­æ–‡)
â”‚   â”œâ”€â”€ Plan_v2.0.md                    # Design plan v2.0 (ä¸­æ–‡)
â”‚   â”œâ”€â”€ initial_plan.md                 # Original methodology
â”‚   â””â”€â”€ initial_plan_update.md          # Updated methodology
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py                      # FlowDataset with triplet sampling
â”‚   â”œâ”€â”€ tokenizer.py                    # BasicTokenizer / ESM tokenizer
â”‚   â””â”€â”€ convert_csv_to_jsonl.py         # Data preprocessing
â”‚
â”œâ”€â”€ common/
â”‚   â””â”€â”€ utils.py                        # Checkpointing, early stopping
â”‚
â”œâ”€â”€ Immuno_PLM/
â”‚   â”œâ”€â”€ immuno_plm.py                   # ImmunoPLM model
â”‚   â”œâ”€â”€ train_plm.py                    # Training script
â”‚   â””â”€â”€ eval_plm.py                     # Evaluation script
â”‚
â”œâ”€â”€ FlowTCR_Gen/
â”‚   â”œâ”€â”€ flow_gen.py                     # FlowMatchingModel
â”‚   â”œâ”€â”€ train_flow.py                   # Training script
â”‚   â””â”€â”€ pipeline_impl.py                # Full inference pipeline
â”‚
â”œâ”€â”€ TCRFold_Light/
â”‚   â”œâ”€â”€ tcrfold_light.py                # TCRFoldLight model
â”‚   â”œâ”€â”€ train_ppi_impl.py               # PPI pretraining
â”‚   â”œâ”€â”€ train_tcr_impl.py               # TCR finetuning
â”‚   â””â”€â”€ train_with_energy.py            # Energy-supervised training
â”‚
â”œâ”€â”€ physics/
â”‚   â”œâ”€â”€ evoef_runner.py                 # EvoEF2 Python wrapper
â”‚   â”œâ”€â”€ energy_dataset.py               # Energy-labeled dataset
â”‚   â”œâ”€â”€ test_evoef.py                   # EvoEF2 tests
â”‚   â””â”€â”€ README.md                       # Physics module docs
â”‚
â””â”€â”€ tools/
    â””â”€â”€ EvoEF2/                         # EvoEF2 binary + params
```

---

## 9. Quickstart Guide

### 9.1 Environment Setup

```bash
# Create environment
conda create -n flowtcr python=3.9
conda activate flowtcr

# Install core dependencies
pip install torch transformers biopython pandas numpy

# Install ESM-2 (required for Immuno-PLM)
pip install fair-esm

# Install PEFT for LoRA (required for ESM-2 fine-tuning)
pip install 

# (Optional) Install wandb for experiment tracking
pip install wandb
```

**Memory Requirements**:
| Mode | VRAM | Notes |
|------|------|-------|
| BasicTokenizer | ~2 GB | For debugging only |
| ESM-2 (frozen) | ~8 GB | Good for testing |
| ESM-2 + LoRA | ~12 GB | Recommended for production |

### 9.2 Data Preparation

```bash
# 1. Prepare training data
head -3 data/trn.csv
# peptide,mhc,cdr3_b,h_v,h_j,l_v,l_j

# 2. Build scaffold bank
python -c "
import pandas as pd
df = pd.read_csv('data/trn.csv')
scaffolds = df.groupby(['h_v','h_j','l_v','l_j']).size().reset_index(name='count')
scaffolds.to_csv('data/scaffold_bank.csv', index=False)
print(f'Unique scaffolds: {len(scaffolds)}')
"

# 3. (Optional) Prepare PDB structures
mkdir -p data/pdb_structures
# Download from STCRDab / TCR3d
```

### 9.3 Training

```bash
# Step 1: Train Immuno-PLM with ESM-2 + LoRA (design target; uses in-house LoRA if no )
python flowtcr_fold/Immuno_PLM/train_plm.py     --data data/trn.csv     --use_esm --use_lora     --esm_model esm2_t33_650M_UR50D     --lora_rank 8     --batch_size 32     --epochs 100     --out_dir checkpoints/plm

# Step 1 (implemented subset): BasicTokenizer or frozen ESM features
python flowtcr_fold/Immuno_PLM/train_plm.py     --data data/trn.csv     --batch_size 32     --epochs 100     --out_dir checkpoints/plm

# Optional: use frozen ESM features if installed
python flowtcr_fold/Immuno_PLM/train_plm.py     --data data/trn.csv     --use_esm     --esm_model esm2_t6_8M_UR50D     --batch_size 32     --epochs 100     --out_dir checkpoints/plm


# Step 2: Train FlowTCR-Gen
python flowtcr_fold/FlowTCR_Gen/train_flow.py \
    --data data/trn.csv \
    --epochs 100

# (Optional) Step 3: Train TCRFold-Light
python flowtcr_fold/TCRFold_Light/train_with_energy.py \
    --pdb_dir data/pdb_structures
```

### 9.4 Inference

```bash
# Run design pipeline
python flowtcr_fold/FlowTCR_Gen/pipeline_impl.py \
    --peptide "GILGFVFTL" \
    --mhc "HLA-A*02:01"
```

---

## 10. Legacy Code References

This project builds upon validated components from previous work:

| Legacy Module | Location | Reused Components |
|---------------|----------|-------------------|
| **Topology Bias** | `conditioned/model.py` | Region/pair embeddings (lines 85-117) |
| **Hierarchical Pairs** | `psi_model/model.py` | `create_hierarchical_pairs`, Collapse token |
| **Evoformer** | `conditioned/src/Evoformer.py` | Triangle updates, Triangle attention |
| **Data Patterns** | `conditioned/data.py` | Masking, amino acid encoding |

---

## 11. Status & Roadmap

### 11.1 Implementation Status

| Module | Status | Notes |
|--------|--------|-------|
| **Data Infrastructure** | âœ… 90% | Triplet sampler, tokenizer, scaffold bank |
| **Immuno-PLM** | âœ… 80% | InfoNCE + topology bias working |
| **FlowTCR-Gen** | ðŸ”„ 40% | Basic flow matching, needs full conditioning |
| **TCRFold-Light** | âœ… 75% | EvoEF2 integration complete |
| **Physics Module** | âœ… 90% | EvoEF2 wrapper fully functional |
| **Inference Pipeline** | ðŸ”„ 50% | Skeleton implemented |

### 11.2 Roadmap

| Phase | Tasks | Priority |
|-------|-------|----------|
| **Phase 1** | Validate Immuno-PLM training (Batch InfoNCE) | ðŸ”´ High |
| **Phase 2** | Implement scaffold retrieval evaluation | ðŸ”´ High |
| **Phase 3** | Complete FlowTCR-Gen with full conditioning | ðŸŸ¡ Medium |
| **Phase 4** | TCRFold-Light training with PDB data | ðŸŸ¡ Medium |
| **Phase 5** | End-to-end pipeline integration | ðŸŸ¡ Medium |
| **Phase 6** | Benchmarking against baselines | ðŸŸ¢ Low |

---

## References

- **EvoEF2**: Huang X, Pearce R, Zhang Y. Bioinformatics (2020), 36:1135-1142
- **ESM-2**: Lin Z, et al. Science (2023)
- **Flow Matching**: Lipman Y, et al. ICLR (2023)
- **psi_model**: Internal development (hierarchical pair embeddings)

---

**Last Updated**: 2025-11-28  
**Version**: 2.0  
**Maintainers**: FlowTCR-Fold Team
