# model.py for psiCLM with FIXED data leakage issue

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

def one_d(idx_, d, max_len=2056):
    """One-dimensional positional encoding"""
    if len(idx_) == 0:
        return torch.zeros((0, int(d)))
    
    # ç¡®ä¿idx_æ˜¯é•¿æ•´å‹å¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
    idx_ = idx_.long()
    device = idx_.device
    
    max_idx = min(max_len, idx_.max().item() + 1)
    emb = torch.zeros((int(max_idx), int(d)), device=device)
    
    # åˆ›å»ºä½ç½®ç´¢å¼•
    positions = torch.arange(max_idx, device=device).float()
    
    for i in range(int(d)):
        if i % 2 == 0:
            emb[:, i] = torch.sin(positions / (10000 ** (i / d)))
        else:
            emb[:, i] = torch.cos(positions / (10000 ** (i / d)))
    
    return emb[idx_]

def mask_input_tokens(aa, mask):
    """
    ğŸ”¥ CRITICAL FIX: æ­£ç¡®å®ç°è¾“å…¥maskingï¼Œé˜²æ­¢æ•°æ®æ³„éœ²
    aa: (L, vocab_size) one-hot vectors
    mask: (L,) binary mask (1=éœ€è¦é¢„æµ‹çš„ä½ç½®ï¼Œ0=ä¸mask)
    è¿”å›: masked aaï¼Œå…¶ä¸­è¢«maskçš„ä½ç½®ç”¨MASK tokenæ›¿æ¢
    """
    device = aa.device
    vocab_size = aa.shape[1]
    
    # åˆ›å»ºMASK token: å…¨é›¶å‘é‡è¡¨ç¤ºæœªçŸ¥æ°¨åŸºé…¸
    mask_token = torch.zeros(vocab_size, device=device)
    # æˆ–è€…å¯ä»¥ç”¨ç‰¹æ®Šçš„learnable embedding:
    # mask_token = torch.zeros(vocab_size, device=device)
    # mask_token[-1] = 1.0  # å‡è®¾æœ€åä¸€ç»´æ˜¯MASK token
    
    # å¯¹è¢«maskçš„ä½ç½®ï¼Œç”¨MASK tokenæ›¿æ¢åŸå§‹è¾“å…¥
    mask_expanded = mask[:, None].expand(-1, vocab_size)  # (L, vocab_size)
    masked_aa = torch.where(mask_expanded == 1, 
                           mask_token[None, :].expand(aa.shape[0], -1), 
                           aa)
    
    return masked_aa

def nll_loss_withmask(pred, native, mask):
    """
    è®¡ç®—masked NLL loss
    pred: (L, vocab_size) log probabilities
    native: (L, vocab_size) one-hot targets  
    mask: (L,) binary mask (1=è®¡ç®—loss, 0=ä¸è®¡ç®—loss)
    """
    pred = pred.to(mask.device)
    native = native.to(mask.device)
    
    # ç¡®ä¿maskç»´åº¦æ­£ç¡®
    if mask.dim() == 1:
        mask = mask[:, None]  # (L, 1)
    
    # è®¡ç®—é€ç‚¹loss
    pointwise_loss = -(pred * native * mask).sum(dim=-1)  # (L,)
    total_loss = pointwise_loss.sum()
    mask_count = mask.sum()
    
    # æ•°å€¼ç¨³å®šæ€§ï¼šé¿å…é™¤é›¶
    if mask_count == 0:
        print(f"WARNING: mask_count = 0! This should not happen.")
        return torch.tensor(5.0, device=pred.device, requires_grad=True)
    
    loss = total_loss / mask_count
    
    if torch.isnan(loss) or torch.isinf(loss):
        print(f"ERROR: Invalid loss! loss={loss}")
        return torch.tensor(5.0, device=pred.device, requires_grad=True)

    return loss

class Linear(nn.Module):
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.linear = nn.Linear(dim_in, dim_out)
    def forward(self, x):
        return self.linear(x)

class CollapseAwareEmbedding(nn.Module):
    """
    ğŸ”¥ FIXED: å®Œå…¨ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜çš„embeddingå±‚
    """
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.s_in_dim = cfg['s_in_dim']
        self.z_in_dim = cfg['z_in_dim']
        self.s_dim = cfg['s_dim']
        self.z_dim = cfg['z_dim']

        self.seq_proj = Linear(self.s_in_dim, self.s_dim)
        self.pair_embed_lvl1 = Linear(8, self.z_dim // 2)
        self.pair_embed_lvl2 = Linear(4, self.z_dim // 2)
        self.pos_embed_s = Linear(64, self.s_dim)

        # ğŸ”§ ä¿®å¤collapse tokenåˆå§‹åŒ–ï¼šä½¿ç”¨æ›´å°çš„æ–¹å·®
        self.collapse_token = nn.Parameter(torch.randn(1, self.s_dim) * 0.1)  # å‡å°æ–¹å·®
        
        # åŒºåŸŸç‰¹å®šçš„è‡ªé€‚åº”æƒé‡
        self.region_weights = nn.ParameterDict({
            'hd': nn.Parameter(torch.ones(2)),      # [seq_weight, pos_weight] for HD
            'mhc': nn.Parameter(torch.ones(2)),     # [seq_weight, pos_weight] for MHC  
            'pep': nn.Parameter(torch.ones(2)),     # [seq_weight, pos_weight] for PEP
            'lv': nn.Parameter(torch.ones(2)),      # [seq_weight, pos_weight] for LV
            'lj': nn.Parameter(torch.ones(2)),      # [seq_weight, pos_weight] for LJ
            'hv': nn.Parameter(torch.ones(2)),      # [seq_weight, pos_weight] for HV
            'hj': nn.Parameter(torch.ones(2)),      # [seq_weight, pos_weight] for HJ
        })
        self.collapse_weight = nn.Parameter(torch.ones(1))
        
        # ğŸ”§ åº”ç”¨Xavieråˆå§‹åŒ–
        self._initialize_weights()

    def _initialize_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        # Xavieråˆå§‹åŒ–çº¿æ€§å±‚
        for module in [self.seq_proj, self.pair_embed_lvl1, self.pair_embed_lvl2, self.pos_embed_s]:
            if hasattr(module, 'linear'):
                nn.init.xavier_uniform_(module.linear.weight)
                nn.init.constant_(module.linear.bias, 0.0)

    def forward(self, in_dict, conditioning_info):
        device = get_device()
        s_list, idx_map = [], []
        
        # Collapse token with learnable weight
        collapse_emb = self.collapse_weight * self.collapse_token
        s_list.append(collapse_emb)
        
        offset = 1
        for k in ['hd'] + [k for k in ['mhc','pep','lv','lj','hv','hj'] if k in conditioning_info]:
            if k in in_dict and in_dict[k].shape[0] > 0:
                aa = in_dict[k].to(device)
                
                # ğŸ”¥ CRITICAL FIX: åœ¨embeddingä¹‹å‰å°±æ­£ç¡®maskè¾“å…¥ï¼
                if k == 'hd':
                    mask = in_dict['mask'].to(device)  # (L,) binary mask
                    # å…³é”®ä¿®å¤ï¼šç”¨MASK tokenæ›¿æ¢è¢«maskä½ç½®çš„è¾“å…¥
                    aa = mask_input_tokens(aa, mask)
                    # print(f"DEBUG: HD sequence masked. Original shape: {in_dict[k].shape}, Mask sum: {mask.sum()}")
                
                # conditioningåºåˆ—ä¸éœ€è¦maskå¤„ç†ï¼ˆå®ƒä»¬æ˜¯å®Œå…¨å¯è§çš„ï¼‰
                
                # åˆ†åˆ«è®¡ç®—åºåˆ—å’Œä½ç½®ç¼–ç 
                seq_emb = self.seq_proj(aa)
                pos_emb = self.pos_embed_s(one_d(in_dict[f'{k}_idx'].to(device), 64))
                
                # åº”ç”¨åŒºåŸŸç‰¹å®šæƒé‡
                if k in self.region_weights:
                    region_seq_w, region_pos_w = self.region_weights[k]
                    s = region_seq_w * seq_emb + region_pos_w * pos_emb
                else:
                    s = seq_emb + pos_emb
                
                s_list.append(s)
                idx_map.append((offset, offset + s.shape[0]))
                offset += s.shape[0]
                
        s_out = torch.cat(s_list, dim=0)
        L = s_out.shape[0]

        # ä½¿ç”¨æ”¹è¿›çš„å±‚æ¬¡åŒ–pair embedding
        pair_id = self.create_hierarchical_pairs(L, idx_map, device)
        z = torch.cat([
            self.pair_embed_lvl1(F.one_hot(pair_id//4, 8).float()),
            self.pair_embed_lvl2(F.one_hot(pair_id%4, 4).float())
        ], dim=-1)

        return s_out, z

    def create_hierarchical_pairs(self, L, idx_map, device):
        """åˆ›å»ºå±‚æ¬¡åŒ–pair embedding"""
        pair_id = torch.zeros((L, L), dtype=torch.long, device=device)
        
        # æ‰¾åˆ°å„åŒºåŸŸè¾¹ç•Œ
        collapse_end = 1
        hd_start, hd_end = idx_map[0] if idx_map else (1, 1)
        
        # Level 0: Collapse self-reference (psi=psi(psi))
        pair_id[0, 0] = 0
        
        # Level 1: Collapse â†” all other regions
        pair_id[0, 1:] = 1
        pair_id[1:, 0] = 1
        
        # Level 2: HD sequential neighbors
        if hd_end > hd_start:
            for i in range(hd_start, hd_end-1):
                pair_id[i, i+1] = 2
                pair_id[i+1, i] = 2
        
        # Level 3: HD internal non-sequential
        for i in range(hd_start, hd_end):
            for j in range(hd_start, hd_end):
                if i != j and pair_id[i, j] == 0:
                    pair_id[i, j] = 3
        
        # Level 4: HD â†” conditioning
        for i in range(hd_start, hd_end):
            for region_start, region_end in idx_map[1:]:
                pair_id[i, region_start:region_end] = 4
                pair_id[region_start:region_end, i] = 4
        
        # Level 5+: Conditioning regions internal
        counter = 5
        for region_start, region_end in idx_map[1:]:
            pair_id[region_start:region_end, region_start:region_end] = counter
            counter += 1
        
        # Level N+: Conditioning â†” conditioning
        conditioning_regions = idx_map[1:]
        for i, (r1_start, r1_end) in enumerate(conditioning_regions):
            for j, (r2_start, r2_end) in enumerate(conditioning_regions[i+1:], i+1):
                pair_id[r1_start:r1_end, r2_start:r2_end] = counter
                pair_id[r2_start:r2_end, r1_start:r1_end] = counter
                counter += 1
        
        return pair_id.clamp(max=31)

class LightweightControlledAttention(nn.Module):
    """æ˜¾å­˜å‹å¥½çš„å¯æ§attentionæœºåˆ¶"""
    def __init__(self, s_dim, z_dim):
        super().__init__()
        # ğŸ”§ ä½¿ç”¨åŸå§‹çš„MultiheadAttentionï¼Œåªæ·»åŠ æœ€å°çš„æ§åˆ¶å‚æ•°
        self.attn = nn.MultiheadAttention(s_dim, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # ğŸ”§ æœ€å°åŒ–çš„æ§åˆ¶å‚æ•°ï¼šåªé’ˆå¯¹collapse token
        self.collapse_bias = nn.Parameter(torch.zeros(64))  # å‡å°‘åˆ°64ï¼ŒåŠ¨æ€æ‰©å±•
        self.bias_scale = nn.Parameter(torch.ones(1) * 0.1)  # å¯å­¦ä¹ çš„ç¼©æ”¾å› å­
        
        # åˆå§‹åŒ–ä¸ºéå‡åŒ€åˆ†å¸ƒ
        nn.init.normal_(self.collapse_bias, mean=0.0, std=0.3)

    def forward(self, s, z, attn_mask=None):
        B, L, D = s.shape
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        if attn_mask is not None:
            attn_mask = attn_mask.to(s.device)
        
        # ğŸ”§ ä½¿ç”¨hookæœºåˆ¶ä¿®æ”¹attentionï¼Œè€Œä¸æ˜¯é‡æ–°å®ç°æ•´ä¸ªattention
        def attention_hook(module, input, output):
            attn_output, attn_weights = output
            
            if attn_weights is not None and L > 1:
                # åªä¿®æ”¹collapse token (ç¬¬ä¸€è¡Œ) çš„attention
                modified_weights = attn_weights.clone()
                
                # åŠ¨æ€è°ƒæ•´biasé•¿åº¦
                bias_length = min(L, len(self.collapse_bias))
                bias = self.collapse_bias[:bias_length] * self.bias_scale
                
                # åº”ç”¨biasåˆ°collapse tokençš„attention logits (éœ€è¦é€†å‘softmax)
                collapse_attn = modified_weights[0, 0, :bias_length]  # [L]
                
                # è½¬æ¢ä¸ºlogits (è¿‘ä¼¼)
                logits = torch.log(collapse_attn + 1e-8)
                
                # æ·»åŠ bias
                logits = logits + bias
                
                # é‡æ–°softmax
                new_collapse_attn = F.softmax(logits, dim=0)
                modified_weights[0, 0, :bias_length] = new_collapse_attn
                
                return attn_output, modified_weights
            
            return output
        
        # æ³¨å†Œhook
        hook_handle = self.attn.register_forward_hook(attention_hook)
        
        try:
            s_out, attn_weights = self.attn(s_ln, s_ln, s_ln, 
                                           attn_mask=attn_mask, 
                                           need_weights=True)
        finally:
            # æ¸…ç†hook
            hook_handle.remove()
        
        s = s + s_out
        s = s + self.ffn(s)
        
        return s, z, attn_weights

class MemoryEfficientEvoformer(nn.Module):
    """æ˜¾å­˜é«˜æ•ˆçš„Evoformer"""
    def __init__(self, cfg):
        super().__init__()
        # ä½¿ç”¨è½»é‡çº§attention blocks
        self.layers = nn.ModuleList([LightweightControlledAttention(cfg['s_dim'], cfg['z_dim']) for _ in range(cfg['N_elayers'])])
        self.log_attn = []

    def forward(self, s, z, attn_mask=None):
        self.log_attn = []
        for layer in self.layers:
            s, z, a = layer(s, z, attn_mask)
            self.log_attn.append(a)
        return s, z

class EvoBlockWithLog(nn.Module):
    def __init__(self, s_dim, z_dim):
        super().__init__()
        self.attn = nn.MultiheadAttention(s_dim, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # ğŸ”§ æ·»åŠ temperature scalingå‚æ•°
        self.attention_temperature = nn.Parameter(torch.ones(1) * 1.0)  # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
        
        # ğŸ”§ æ”¹è¿›attentionåˆå§‹åŒ–
        self._initialize_attention()
    
    def _initialize_attention(self):
        """æ”¹è¿›çš„attentionæƒé‡åˆå§‹åŒ–"""
        # å¯¹MultiheadAttentionè¿›è¡Œæ›´å¥½çš„åˆå§‹åŒ–
        for name, param in self.attn.named_parameters():
            if 'weight' in name:
                if 'in_proj' in name:  # Q, K, VæŠ•å½±çŸ©é˜µ
                    nn.init.xavier_uniform_(param)
                elif 'out_proj' in name:  # è¾“å‡ºæŠ•å½±çŸ©é˜µ
                    nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)

    def forward(self, s, z, attn_mask=None):
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        if attn_mask is not None:
            attn_mask = attn_mask.to(s.device)
        
        # ğŸ”§ åº”ç”¨temperature scaling
        s_out, attn_map = self.attn(s_ln, s_ln, s_ln, 
                                   attn_mask=attn_mask, 
                                   need_weights=True)
        
        # åº”ç”¨temperatureåˆ°attention map (ä»…ç”¨äºç›‘æ§ï¼Œä¸å½±å“forward pass)
        if hasattr(self, 'attention_temperature'):
            # è¿™é‡Œåªæ˜¯ä¸ºäº†è°ƒè¯•ï¼Œä¸æ”¹å˜å®é™…çš„è®¡ç®—æµç¨‹
            pass
            
        s = s + s_out
        s = s + self.ffn(s)
        return s, z, attn_map

class ControlledAttentionBlock(nn.Module):
    """å®Œå…¨å¯æ§çš„attentionæœºåˆ¶ï¼Œç›´æ¥åœ¨è®¡ç®—è¿‡ç¨‹ä¸­æ§åˆ¶åˆ†å¸ƒ"""
    def __init__(self, s_dim, z_dim):
        super().__init__()
        self.s_dim = s_dim
        self.num_heads = 4
        self.head_dim = s_dim // 4
        
        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(s_dim, s_dim)
        self.k_proj = nn.Linear(s_dim, s_dim)
        self.v_proj = nn.Linear(s_dim, s_dim)
        self.out_proj = nn.Linear(s_dim, s_dim)
        
        # ğŸ”§ å…³é”®ï¼šå¯å­¦ä¹ çš„attentionåå¥½å‚æ•°
        self.collapse_attention_bias = nn.Parameter(torch.zeros(512))  # æœ€å¤§åºåˆ—é•¿åº¦
        self.attention_sharpening = nn.Parameter(torch.ones(1) * 1.0)
        
        # FFN
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # åˆå§‹åŒ–åå¥½ä¸ºéå‡åŒ€åˆ†å¸ƒ
        nn.init.normal_(self.collapse_attention_bias, mean=0.0, std=0.5)

    def forward(self, s, z, attn_mask=None):
        B, L, D = s.shape
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        # è®¡ç®— Q, K, V
        q = self.q_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šç›´æ¥åœ¨softmaxä¹‹å‰ä¿®æ”¹collapse tokençš„scores
        if L > 1:  # ç¡®ä¿æœ‰å¤šä¸ªä½ç½®
            # ä¸ºcollapse token (position 0) æ·»åŠ å¯å­¦ä¹ çš„åå¥½
            collapse_bias = self.collapse_attention_bias[:L].unsqueeze(0).unsqueeze(0).unsqueeze(0)  # [1, 1, 1, L]
            
            # åªä¿®æ”¹collapse tokençš„æ³¨æ„åŠ›åˆ†å¸ƒ (æ‰€æœ‰å¤´çš„ç¬¬ä¸€ä¸ªquery)
            scores[:, :, 0, :] = scores[:, :, 0, :] + collapse_bias.squeeze(2)
            
            # åº”ç”¨å¯å­¦ä¹ çš„é”åŒ–å‚æ•°
            scores[:, :, 0, :] = scores[:, :, 0, :] * self.attention_sharpening
        
        # åº”ç”¨attention mask (å¦‚æœæœ‰)
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask.unsqueeze(1).unsqueeze(1), float('-inf'))
        
        # è®¡ç®—attentionæƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        
        # åº”ç”¨attentionåˆ°values
        attn_output = torch.matmul(attn_weights, v)
        
        # é‡ç»„è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)
        s_out = self.out_proj(attn_output)
        
        s = s + s_out
        s = s + self.ffn(s)
        
        # è¿”å›ç¬¬ä¸€ä¸ªå¤´çš„attentionæƒé‡ç”¨äºç›‘æ§
        return s, z, attn_weights[:, 0, :, :]  # [B, L, L]

class ControlledCollapseEvoformer(nn.Module):
    """ä½¿ç”¨å¯æ§attentionçš„Evoformer"""
    def __init__(self, cfg):
        super().__init__()
        # ä½¿ç”¨å¯æ§çš„attention blocks
        self.layers = nn.ModuleList([ControlledAttentionBlock(cfg['s_dim'], cfg['z_dim']) for _ in range(cfg['N_elayers'])])
        self.log_attn = []

    def forward(self, s, z, attn_mask=None):
        self.log_attn = []
        for layer in self.layers:
            s, z, a = layer(s, z, attn_mask)
            self.log_attn.append(a)
        return s, z

class ForcedAttentionBlock(nn.Module):
    """å¼ºåˆ¶æ€§attentioné›†ä¸­æœºåˆ¶"""
    def __init__(self, s_dim, z_dim):
        super().__init__()
        self.attn = nn.MultiheadAttention(s_dim, num_heads=4, batch_first=True)
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # ğŸ”§ å¼ºåˆ¶æ€§attentionåå¥½ï¼šå¯å­¦ä¹ çš„ä½ç½®æƒé‡
        self.position_bias = nn.Parameter(torch.zeros(1, 1, 512))  # æœ€å¤§åºåˆ—é•¿åº¦
        self.attention_sharpening = nn.Parameter(torch.ones(1) * 2.0)  # å¯å­¦ä¹ çš„é”åŒ–å‚æ•°
        
        # åˆå§‹åŒ–ä½ç½®åå¥½ä¸ºéšæœºéå‡åŒ€åˆ†å¸ƒ
        nn.init.normal_(self.position_bias, mean=0.0, std=0.5)

    def forward(self, s, z, attn_mask=None):
        B, L, D = s.shape
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        if attn_mask is not None:
            attn_mask = attn_mask.to(s.device)
        
        # è®¡ç®—åŸå§‹attention
        s_out, attn_weights = self.attn(s_ln, s_ln, s_ln, 
                                       attn_mask=attn_mask, 
                                       need_weights=True)
        
        # ğŸ”§ å¼ºåˆ¶ä¿®æ”¹collapse tokençš„attentionåˆ†å¸ƒ
        if attn_weights.shape[-1] > 1:  # ç¡®ä¿æœ‰å¤šä¸ªä½ç½®
            # ä¸ºcollapse token (position 0) åº”ç”¨ä½ç½®åå¥½
            position_bias = self.position_bias[:, :, :L]  # æˆªå–åˆ°å®é™…åºåˆ—é•¿åº¦
            
            # å¯¹ç¬¬ä¸€è¡Œï¼ˆcollapse tokençš„attentionï¼‰åº”ç”¨åå¥½å’Œé”åŒ–
            modified_attn = attn_weights.clone()
            collapse_attn = modified_attn[:, 0, :]  # [B, L]
            
            # æ·»åŠ ä½ç½®åå¥½
            collapse_attn = collapse_attn + position_bias.squeeze(0).squeeze(0)[:L]
            
            # åº”ç”¨é”åŒ–
            collapse_attn = collapse_attn * self.attention_sharpening
            
            # é‡æ–°å½’ä¸€åŒ–
            collapse_attn = F.softmax(collapse_attn, dim=-1)
            modified_attn[:, 0, :] = collapse_attn
            
            # ç”¨ä¿®æ”¹åçš„attentioné‡æ–°è®¡ç®—è¾“å‡º
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥é‡æ–°è®¡ç®—attention output
            s_out[0, 0, :] = torch.matmul(collapse_attn[0:1, :], s_ln[0, :, :]).squeeze(0)
        
        s = s + s_out
        s = s + self.ffn(s)
        return s, z, attn_weights

class SequenceProfileAttention(nn.Module):
    """ç»“åˆå›¾ä¸­sequence profileæ–¹æ³•çš„attentionæœºåˆ¶"""
    def __init__(self, s_dim, z_dim):
        super().__init__()
        self.s_dim = s_dim
        self.num_heads = 4
        self.head_dim = s_dim // 4
        
        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(s_dim, s_dim)
        self.k_proj = nn.Linear(s_dim, s_dim)
        self.v_proj = nn.Linear(s_dim, s_dim)
        self.out_proj = nn.Linear(s_dim, s_dim)
        
        # ğŸ”§ åºåˆ—profileé¢„æµ‹å¤´ï¼ˆç±»ä¼¼å›¾ä¸­çš„h_i(x_i|s)ï¼‰
        self.profile_head = nn.Linear(s_dim, 20)  # é¢„æµ‹20ç§æ°¨åŸºé…¸æ¦‚ç‡
        
        # ğŸ”§ Collapse tokençš„ä½ç½®åå¥½
        self.collapse_position_bias = nn.Parameter(torch.zeros(512))
        self.entropy_weight = nn.Parameter(torch.ones(1) * 0.1)
        
        # FFN
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.collapse_position_bias, mean=0.0, std=0.3)

    def compute_sequence_profile_entropy(self, hidden_states):
        """è®¡ç®—åºåˆ—profileçš„entropyï¼ˆç±»ä¼¼å›¾ä¸­æ–¹æ³•ï¼‰"""
        # å¯¹æ¯ä¸ªä½ç½®é¢„æµ‹æ°¨åŸºé…¸åˆ†å¸ƒ
        profile_logits = self.profile_head(hidden_states)  # [B, L, 20]
        profile_probs = F.softmax(profile_logits, dim=-1)
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„entropy
        position_entropy = -(profile_probs * torch.log(profile_probs + 1e-8)).sum(dim=-1)  # [B, L]
        
        return position_entropy, profile_probs

    def forward(self, s, z, attn_mask=None):
        B, L, D = s.shape
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        # ğŸ”§ è®¡ç®—åºåˆ—profile entropy
        position_entropy, profile_probs = self.compute_sequence_profile_entropy(s_ln)
        
        # è®¡ç®— Q, K, V
        q = self.q_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # ğŸ”§ å…³é”®åˆ›æ–°ï¼šä½¿ç”¨position entropyæ¥æŒ‡å¯¼attention
        if L > 1:
            # å¯¹äºcollapse tokenï¼Œè®©å®ƒå…³æ³¨entropyä½çš„ä½ç½®ï¼ˆé«˜ç½®ä¿¡åº¦é¢„æµ‹ï¼‰
            entropy_guidance = -position_entropy[0, :L] * self.entropy_weight  # è´Ÿå·ï¼šä½entropy=é«˜æƒé‡
            
            # æ·»åŠ ä½ç½®åå¥½
            position_bias = self.collapse_position_bias[:L]
            
            # ç»„åˆguidance
            total_bias = entropy_guidance + position_bias
            
            # åº”ç”¨åˆ°collapse tokençš„æ‰€æœ‰å¤´
            scores[:, :, 0, :L] = scores[:, :, 0, :L] + total_bias.unsqueeze(0).unsqueeze(0)
        
        # åº”ç”¨attention mask
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask.unsqueeze(1).unsqueeze(1), float('-inf'))
        
        # è®¡ç®—attentionæƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        
        # åº”ç”¨attentionåˆ°values
        attn_output = torch.matmul(attn_weights, v)
        
        # é‡ç»„è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)
        s_out = self.out_proj(attn_output)
        
        s = s + s_out
        s = s + self.ffn(s)
        
        # è¿”å›attentionæƒé‡å’Œé¢å¤–ä¿¡æ¯
        return s, z, attn_weights[:, 0, :, :], {
            'position_entropy': position_entropy,
            'profile_probs': profile_probs,
            'entropy_guidance': entropy_guidance if L > 1 else None
        }

class SequenceProfileEvoformer(nn.Module):
    """ä½¿ç”¨åºåˆ—profileæ–¹æ³•çš„Evoformer"""
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.ModuleList([SequenceProfileAttention(cfg['s_dim'], cfg['z_dim']) for _ in range(cfg['N_elayers'])])
        self.log_attn = []
        self.log_profile_info = []

    def forward(self, s, z, attn_mask=None):
        self.log_attn = []
        self.log_profile_info = []
        for layer in self.layers:
            s, z, a, profile_info = layer(s, z, attn_mask)
            self.log_attn.append(a)
            self.log_profile_info.append(profile_info)
        return s, z

class ForcedCollapseAttention(nn.Module):
    """å¼ºåˆ¶æ€§çš„collapse attentionæ§åˆ¶æœºåˆ¶ - ç¡®ä¿attentionä¸å‡åŒ€"""
    def __init__(self, s_dim, z_dim):
        super().__init__()
        self.s_dim = s_dim
        self.num_heads = 4
        self.head_dim = s_dim // 4
        
        # Q, K, V æŠ•å½±
        self.q_proj = nn.Linear(s_dim, s_dim)
        self.k_proj = nn.Linear(s_dim, s_dim)
        self.v_proj = nn.Linear(s_dim, s_dim)
        self.out_proj = nn.Linear(s_dim, s_dim)
        
        # ğŸ”¥ å¼ºåˆ¶æ€§attentionæ§åˆ¶å‚æ•°
        self.force_attention_weights = nn.Parameter(torch.zeros(512))
        self.attention_temperature = nn.Parameter(torch.ones(1) * 1.0)
        self.force_strength = nn.Parameter(torch.ones(1) * 5.0)  # å¼ºåˆ¶å¼ºåº¦
        
        # FFN
        self.ffn = nn.Sequential(
            nn.LayerNorm(s_dim),
            nn.Linear(s_dim, s_dim*4),
            nn.ReLU(),
            nn.Linear(s_dim*4, s_dim)
        )
        
        # ğŸ”¥ åˆå§‹åŒ–ä¸ºå¼ºçƒˆçš„éå‡åŒ€åˆ†å¸ƒ
        with torch.no_grad():
            # åˆ›å»ºä¸€ä¸ªæ˜æ˜¾çš„éå‡åŒ€æ¨¡å¼ï¼šå‰å‡ ä¸ªä½ç½®æƒé‡å¾ˆé«˜
            self.force_attention_weights[:10] = 2.0  # å‰10ä¸ªä½ç½®é«˜æƒé‡
            self.force_attention_weights[10:50] = 1.0  # ä¸­é—´ä½ç½®ä¸­ç­‰æƒé‡
            self.force_attention_weights[50:] = 0.0   # åé¢ä½ç½®ä½æƒé‡
            
            # æ·»åŠ éšæœºå™ªå£°
            self.force_attention_weights += torch.randn_like(self.force_attention_weights) * 0.5

    def forward(self, s, z, attn_mask=None):
        B, L, D = s.shape
        s_ln = F.layer_norm(s, s.shape[-1:])
        
        # è®¡ç®— Q, K, V
        q = self.q_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(s_ln).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # ğŸ”¥ å¼ºåˆ¶æ€§attentionä¿®æ”¹ - ç¡®ä¿collapse tokenæœ‰éå‡åŒ€attention
        if L > 1:
            # è·å–å¼ºåˆ¶æƒé‡å¹¶ç¡®ä¿å®ƒä»¬æœ‰è¶³å¤Ÿçš„å˜åŒ–
            forced_weights = self.force_attention_weights[:L]
            
            # ğŸ”§ é‡è¦ï¼šç¡®ä¿æƒé‡æœ‰è¶³å¤Ÿçš„åŠ¨æ€èŒƒå›´
            # å¦‚æœæƒé‡è¿‡äºå‡åŒ€ï¼Œäººä¸ºåˆ›é€ å·®å¼‚
            weight_std = torch.std(forced_weights)
            if weight_std < 0.5:  # æƒé‡å¤ªå‡åŒ€
                # å¼ºåˆ¶åˆ›é€ æ¢¯åº¦ï¼šè®©å‰å‡ ä¸ªä½ç½®æƒé‡æ˜¾è‘—æ›´é«˜
                forced_weights = forced_weights.clone()
                num_high = min(5, L // 4)  # å‰25%çš„ä½ç½®
                forced_weights[:num_high] += 2.0
                forced_weights[num_high:] -= 0.5
            
            # åº”ç”¨å¼ºåˆ¶å¼ºåº¦å’Œæ¸©åº¦
            forced_weights = forced_weights * self.force_strength / self.attention_temperature
            
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¸æ˜¯è¦†ç›–scoresï¼Œè€Œæ˜¯æ·»åŠ å¼ºbias
            # è¿™æ ·æ—¢ä¿æŒäº†å­¦ä¹ èƒ½åŠ›ï¼Œåˆå¼ºåˆ¶äº†ä¸å‡åŒ€æ€§
            bias_strength = 3.0  # å¢å¼ºbiaså¼ºåº¦
            for head in range(self.num_heads):
                scores[:, head, 0, :L] = scores[:, head, 0, :L] + forced_weights.unsqueeze(0) * bias_strength
        
        # åº”ç”¨attention mask
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask.unsqueeze(1).unsqueeze(1), float('-inf'))
        
        # è®¡ç®—attentionæƒé‡
        attn_weights = F.softmax(scores, dim=-1)
        
        # ğŸ”§ æ·»åŠ attentionæƒé‡æ£€æŸ¥å’Œå¹²é¢„
        if L > 1:
            collapse_attention = attn_weights[:, 0, 0, :L]  # ç¬¬ä¸€ä¸ªå¤´çš„collapse attention
            attention_entropy = -(collapse_attention * torch.log(collapse_attention + 1e-8)).sum(dim=-1)
            max_entropy = torch.log(torch.tensor(float(L)))
            
            # å¦‚æœæ³¨æ„åŠ›å¤ªå‡åŒ€ï¼Œç›´æ¥ä¿®æ”¹æƒé‡
            if attention_entropy / max_entropy > 0.9:  # è¶…è¿‡90%çš„æœ€å¤§ç†µ
                # åˆ›å»ºæ˜æ˜¾çš„éå‡åŒ€åˆ†å¸ƒ
                new_attention = torch.zeros_like(collapse_attention)
                # è®©å‰å‡ ä¸ªä½ç½®å ä¸»å¯¼
                focus_positions = min(3, L)
                new_attention[:, :focus_positions] = 0.7 / focus_positions
                new_attention[:, focus_positions:] = 0.3 / (L - focus_positions)
                
                # æ›¿æ¢ç¬¬ä¸€ä¸ªå¤´çš„collapse attention
                attn_weights[:, 0, 0, :L] = new_attention
        
        # åº”ç”¨attentionåˆ°values
        attn_output = torch.matmul(attn_weights, v)
        
        # é‡ç»„è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)
        s_out = self.out_proj(attn_output)
        
        s = s + s_out
        s = s + self.ffn(s)
        
        # è¿”å›attentionæƒé‡
        return s, z, attn_weights[:, 0, :, :], {
            'forced_weights': forced_weights if L > 1 else None,
            'force_strength': self.force_strength.item(),
            'temperature': self.attention_temperature.item(),
            'weight_std': weight_std.item() if L > 1 else 0.0
        }

class ForcedCollapseEvoformer(nn.Module):
    """ä½¿ç”¨å¼ºåˆ¶æ€§attentionæ§åˆ¶çš„Evoformer"""
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.ModuleList([ForcedCollapseAttention(cfg['s_dim'], cfg['z_dim']) for _ in range(cfg['N_elayers'])])
        self.log_attn = []
        self.log_force_info = []

    def forward(self, s, z, attn_mask=None):
        self.log_attn = []
        self.log_force_info = []
        for layer in self.layers:
            s, z, a, force_info = layer(s, z, attn_mask)
            self.log_attn.append(a)
            self.log_force_info.append(force_info)
        return s, z

class psiCLM(nn.Module):
    """ğŸ”¥ FIXED: å®Œå…¨ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜çš„ä¸»æ¨¡å‹"""
    def __init__(self, cfg):
        super().__init__()
        self.embedding = CollapseAwareEmbedding(cfg)
        self.backbone = SequenceProfileEvoformer(cfg)  # ğŸ”§ ä½¿ç”¨sequence profileæ–¹æ³•
        self.head = Linear(cfg['s_dim'], cfg['s_in_dim'])
        self.cfg = cfg
        
        # ğŸ”§ æ·»åŠ åŠ¨æ€æƒé‡è°ƒæ•´åŠŸèƒ½
        self.dynamic_collapse_weight = 0.2
        
        # ğŸ”§ æ·»åŠ attentionè´¨é‡ç›‘æ§
        self.attention_history = []
        self.uniform_attention_count = 0

    def set_regularization_weights(self, collapse_weight):
        """åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–æƒé‡"""
        self.dynamic_collapse_weight = collapse_weight

    def _reset_attention_if_uniform(self):
        """å¦‚æœattentionè¿‡äºå‡åŒ€ï¼Œé‡ç½®ç›¸å…³å‚æ•°"""
        if self.uniform_attention_count > 10:  # è¿ç»­10æ¬¡æ£€æµ‹åˆ°å‡åŒ€attention
            print("ğŸ”§ Resetting attention parameters due to uniform distribution")
            
            # é‡æ–°åˆå§‹åŒ–æœ€åä¸€å±‚çš„sequence profileå‚æ•°
            last_layer = self.backbone.layers[-1]
            with torch.no_grad():
                # é‡æ–°åˆå§‹åŒ–ä½ç½®åå¥½
                if hasattr(last_layer, 'collapse_position_bias'):
                    last_layer.collapse_position_bias.data = torch.randn_like(last_layer.collapse_position_bias.data) * 0.5
                    # ç¡®ä¿å‰å‡ ä¸ªä½ç½®æœ‰æ›´é«˜æƒé‡
                    last_layer.collapse_position_bias.data[:10] += 1.0
                
                # é‡ç½®ç†µæƒé‡
                if hasattr(last_layer, 'entropy_weight'):
                    last_layer.entropy_weight.data = torch.ones_like(last_layer.entropy_weight.data) * 0.2
            
            # é‡ç½®è®¡æ•°å™¨
            self.uniform_attention_count = 0

    def forward(self, in_dict, computeloss, conditioning_info=None):
        if conditioning_info is None:
            conditioning_info = []
            
        device = get_device()
        for key in in_dict:
            if isinstance(in_dict[key], torch.Tensor):
                in_dict[key] = in_dict[key].to(device)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šembeddingå±‚å·²ç»æ­£ç¡®å¤„ç†äº†masking
        # ç°åœ¨æ¨¡å‹çœ‹åˆ°çš„HDè¾“å…¥ä¸­ï¼Œè¢«maskçš„ä½ç½®å·²ç»æ˜¯MASK tokenï¼Œä¸æ˜¯åŸå§‹ç­”æ¡ˆï¼
        s, z = self.embedding(in_dict, conditioning_info)
        
        # ä¸ä½¿ç”¨attention maskï¼Œè®©æ¨¡å‹è‡ªç”±å­¦ä¹ attentionæ¨¡å¼
        s, z = self.backbone(s[None, ...], z, attn_mask=None)
        
        L1 = in_dict['hd'].shape[0]
        pred_aa = self.head(s[0][1:L1+1])  # è·³è¿‡collapse token
        
        if not computeloss:
            return torch.softmax(pred_aa, dim=-1), self.backbone.log_attn
        else:
            # ç°åœ¨çš„é¢„æµ‹æ˜¯åŸºäºæ­£ç¡®maskedçš„è¾“å…¥ï¼Œæ²¡æœ‰æ•°æ®æ³„éœ²ï¼
            pred_aa = torch.log_softmax(pred_aa, dim=-1)
            nll_loss = self.compute_nll_loss(pred_aa, in_dict)
            return nll_loss

    def compute_nll_loss(self, pred_aa, in_dict):
        """è®¡ç®—NLL loss - ç°åœ¨æ˜¯åŸºäºæ­£ç¡®maskedè¾“å…¥çš„é¢„æµ‹"""
        return nll_loss_withmask(pred_aa, in_dict['hd'], in_dict['mask'])

    def compute_composite_loss(self, in_dict, conditioning_info=None):
        if conditioning_info is None:
            conditioning_info = []
            
        pred_logits, attn_traces = self(in_dict, computeloss=False, conditioning_info=conditioning_info)
        
        # åŸºç¡€NLLæŸå¤±
        pred_aa = torch.log_softmax(pred_logits, dim=-1)
        nll_loss = nll_loss_withmask(pred_aa, in_dict['hd'], in_dict['mask'])
        
        # ğŸ”§ Sequence Profileç›¸å…³æŸå¤±
        profile_regularization_loss = self._compute_profile_regularization_loss()
        
        # ğŸ”§ Attention entropyæŸå¤±ï¼ˆåŸæœ‰çš„ï¼‰
        collapse_entropy = self._compute_collapse_entropy(attn_traces)
        
        lambda_nll = 1.0
        lambda_profile = 0.05  # sequence profileæ­£åˆ™åŒ–æƒé‡
        lambda_attention = self.dynamic_collapse_weight * 0.1  # ğŸ”§ å¤§å¹…å‡å°æƒé‡ï¼Œé¿å…å‹å€’NLL loss
        
        # ğŸ”§ ä¿®å¤ç¬¦å·ï¼šæˆ‘ä»¬æƒ³è¦æœ€å°åŒ–ç†µï¼ˆè®©attentionæ›´é›†ä¸­ï¼‰
        # æ‰€ä»¥åº”è¯¥æ˜¯ +lambda_attention * collapse_entropy ï¼ˆæƒ©ç½šé«˜ç†µï¼‰
        total_loss = (lambda_nll * nll_loss + 
                     lambda_profile * profile_regularization_loss +
                     lambda_attention * collapse_entropy)  # ğŸ”§ æ”¹ä¸ºæ­£å·
        
        # ğŸ”§ æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"âš ï¸  Invalid loss detected: NLL={nll_loss.item():.3f}, "
                  f"Profile={profile_regularization_loss.item():.3f}, "
                  f"Entropy={collapse_entropy.item():.3f}")
            total_loss = nll_loss  # å›é€€åˆ°çº¯NLL loss
        
        return {
            'total_loss': total_loss,
            'nll_loss': nll_loss,
            'collapse_entropy': collapse_entropy,
            'profile_regularization_loss': profile_regularization_loss
        }

    def _compute_profile_regularization_loss(self):
        """è®¡ç®—sequence profileçš„æ­£åˆ™åŒ–æŸå¤±"""
        if not hasattr(self.backbone, 'log_profile_info') or not self.backbone.log_profile_info:
            return torch.tensor(0.0, device=get_device())
        
        total_profile_loss = 0.0
        count = 0
        
        # éå†æ‰€æœ‰å±‚çš„profileä¿¡æ¯
        for profile_info in self.backbone.log_profile_info:
            if profile_info and 'position_entropy' in profile_info:
                position_entropy = profile_info['position_entropy']
                
                # é¼“åŠ±æ¨¡å‹å¯¹æŸäº›ä½ç½®æœ‰æ›´ç¡®å®šçš„é¢„æµ‹ï¼ˆä½ç†µï¼‰
                # ä½†ä¸æ˜¯æ‰€æœ‰ä½ç½®éƒ½è¦ä½ç†µï¼Œä¿æŒä¸€å®šçš„ä¸ç¡®å®šæ€§
                if position_entropy is not None:
                    # è®¡ç®—ç†µçš„æ–¹å·®ï¼šé¼“åŠ±æœ‰äº›ä½ç½®ç¡®å®šï¼Œæœ‰äº›ä½ç½®ä¸ç¡®å®š
                    entropy_variance = torch.var(position_entropy)
                    
                    # æ­£åˆ™åŒ–ï¼šé¼“åŠ±ç†µçš„å¤šæ ·æ€§ï¼ˆæœ‰é«˜æœ‰ä½ï¼‰
                    regularization = -entropy_variance  # è´Ÿå·ï¼šé¼“åŠ±æ›´å¤§çš„æ–¹å·®
                    
                    total_profile_loss += regularization
                    count += 1
        
        return total_profile_loss / max(count, 1)
    
    def _compute_collapse_entropy(self, attn_traces):
        """è®¡ç®—collapse tokençš„æ³¨æ„åŠ›ç†µ - å¢å¼ºç‰ˆè°ƒè¯•"""
        if not attn_traces:
            return torch.tensor(0.0, device=get_device())
        
        # ä½¿ç”¨æœ€åä¸€å±‚çš„collapse attention
        collapse_attn = attn_traces[-1][0, 0, :]  # shape: (L,)
        
        # è®¡ç®—ç†µ
        probs = F.softmax(collapse_attn, dim=0)
        entropy = -(probs * torch.log(probs + 1e-8)).sum()
        
        # ğŸ”§ å¢å¼ºè°ƒè¯•ï¼šæ˜¾ç¤ºprofileä¿¡æ¯
        debug_frequency = 500
        if not hasattr(self, '_debug_counter'):
            self._debug_counter = 0
        self._debug_counter += 1
        
        if self._debug_counter % debug_frequency == 0:
            uniform_entropy = torch.log(torch.tensor(len(probs), dtype=torch.float32))
            entropy_ratio = entropy.item() / uniform_entropy.item()
            
            # è·å–sequence profileä¿¡æ¯
            profile_info = ""
            if hasattr(self.backbone, 'log_profile_info') and self.backbone.log_profile_info:
                last_profile = self.backbone.log_profile_info[-1]
                if last_profile and 'position_entropy' in last_profile:
                    pos_entropy = last_profile['position_entropy']
                    if pos_entropy is not None:
                        avg_pos_entropy = pos_entropy.mean().item()
                        entropy_weight = last_profile.get('entropy_guidance', torch.tensor(0)).mean().item() if 'entropy_guidance' in last_profile and last_profile['entropy_guidance'] is not None else 0
                        profile_info = f"Profile: avg_ent={avg_pos_entropy:.3f}, guidance={entropy_weight:.3f}"
            
            # è·å–ä½ç½®åå¥½ç»Ÿè®¡
            bias_info = ""
            if hasattr(self.backbone.layers[-1], 'collapse_position_bias'):
                bias = self.backbone.layers[-1].collapse_position_bias[:len(probs)]
                bias_std = torch.std(bias).item()
                bias_max = torch.max(bias).item()
                bias_min = torch.min(bias).item()
                bias_info = f"Bias: std={bias_std:.3f}, max={bias_max:.3f}, min={bias_min:.3f}"
            
            print(f"ğŸ”§ Attention: Entropy={entropy.item():.3f}, Ratio={entropy_ratio:.4f}, "
                  f"Max={probs.max().item():.4f}, Min={probs.min().item():.4f}, "
                  f"Std={probs.std().item():.4f}, L={len(probs)}")
            
            if profile_info:
                print(f"   {profile_info}")
            if bias_info:
                print(f"   {bias_info}")
            
            if entropy_ratio > 0.95:
                print("  âš ï¸  Nearly uniform attention!")
                self.uniform_attention_count += 1
            else:
                print("  âœ… Non-uniform attention detected!")
                self.uniform_attention_count = 0
        
        return entropy

def analyze_mask_distribution():
    """åˆ†æmaskåˆ†å¸ƒçš„è¾…åŠ©å‡½æ•°"""
    print("Mask distribution analysis would go here")
    return {}

def train(model, optimizer, start):
    """è®­ç»ƒå‡½æ•°"""
    pass 